{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VectorizeAssignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd13f569360b419a8b5e646c2efaff6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ad14abf4b0094727b4f80b1d3e525c03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d347893b01f94b01924ba44b21f37ff2",
              "IPY_MODEL_848b38c08e194923a05a7ef9f05ce7f9"
            ]
          }
        },
        "ad14abf4b0094727b4f80b1d3e525c03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d347893b01f94b01924ba44b21f37ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_0b81e30c4c364e63aa8b4d28a904607f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ca3adb1a208443abc78fce17f85896e"
          }
        },
        "848b38c08e194923a05a7ef9f05ce7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce19490681f34ce1a6cb4fef3d1c56db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_106676763303483a8e5c705abb61cd08"
          }
        },
        "0b81e30c4c364e63aa8b4d28a904607f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ca3adb1a208443abc78fce17f85896e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce19490681f34ce1a6cb4fef3d1c56db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "106676763303483a8e5c705abb61cd08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b5ab8fe81c248b0b9e3b5df324f3d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d095224745734cf6afc33586ca6238f9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1378e4fd092f48d585e52c945d71cca4",
              "IPY_MODEL_d16f16ff8359445889d2f0c45ed1e385"
            ]
          }
        },
        "d095224745734cf6afc33586ca6238f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1378e4fd092f48d585e52c945d71cca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_c07bb1a8480a40f3b695fa3d87a9d606",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5aa85d8732094e4fb3dff6a757e8490d"
          }
        },
        "d16f16ff8359445889d2f0c45ed1e385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_520b895e0c4b4304bad644188722c90f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_851c6db52e5544caa15446e3f83ca045"
          }
        },
        "c07bb1a8480a40f3b695fa3d87a9d606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5aa85d8732094e4fb3dff6a757e8490d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "520b895e0c4b4304bad644188722c90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "851c6db52e5544caa15446e3f83ca045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbc8bbc-bc91-4f20-82db-a425f654b632"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\r\u001b[K     |▏                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 28.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 22.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 26.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 25.2MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 27.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 19.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 18.1MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 18.1MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 163kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 18.1MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 18.1MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 204kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 235kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 18.1MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 276kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 296kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 307kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 327kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 348kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 368kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 378kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 389kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 399kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 409kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 419kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 440kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 460kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 471kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 481kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 491kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 501kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 522kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 532kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 542kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 552kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 563kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 583kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 593kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 604kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 614kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 624kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 634kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 645kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 655kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 665kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 675kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 686kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 696kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 706kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 716kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 727kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 737kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 747kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 757kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 778kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 788kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 798kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 808kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 819kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 829kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 839kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 849kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 860kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 870kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 880kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 890kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 901kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 911kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 921kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 931kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 942kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 952kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 962kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 972kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 983kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 993kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=fd34b10e6301ea2493c4dc2037850abfcf36cac253d3fffe657aea7215971663\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=bb151e90ada51b0dd33d19cfb1cdddc850d0c0aa5458a6c726bc62b29370f930\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: configparser, sentry-sdk, smmap, gitdb, GitPython, pathtools, shortuuid, docker-pycreds, subprocess32, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d5abec-ba20-4097-ff1f-39b142e49f37"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "\n",
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "# getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)\n",
        "# run.finish()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def relu(X):\n",
        "  # Calculates the Rectified Linear Unit (ReLU) function\n",
        "  return np.maximum(X,0)\n",
        "\n",
        "def sigmoid(X):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -X))\n",
        "\n",
        "def softmax(X):\n",
        "  # Calculates the softmax function\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "def tanh(X):\n",
        "  return np.tanh(X)\n",
        "\n",
        "def linear(W, X, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ X + b\n",
        "\n",
        "def grad_relu(X):\n",
        "  # Calculates the gradient of Rectified Linear Unit (ReLU) function\n",
        "  return X > 0\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(X) * (1 - sigmoid(X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(X)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(Y_pred, Y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(Y - Y_pred)\n",
        "\n",
        "def Softmax_SquaredError_grad(Y_pred, Y):\n",
        "  return ((Y_pred - Y) - ((Y_pred - Y) * Y_pred).sum(axis = 0)) * Y_pred "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "  # Initialising a matrix by xavier initialisation with given dimensions (shape) as tuple\n",
        "  bound = (6/(shape[0]+shape[1]))**(0.5)\n",
        "  return bound*(2*np.random.rand(*shape)-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = H_prev                    # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  cache = (pre_activation_cache, A)         # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, X):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      X -- Input data from the training set as a matrix with ith column representing ith input training data\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network as a matrix with ith column representing output of ith input data\n",
        "    \"\"\"\n",
        "\n",
        "    H = X                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "\n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "        \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, Y, Y_pred, weight_decay, grad_reglr_fn):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      Y -- True output matrix with ith column representing true output of ith input data\n",
        "      Y_pred -- Output of neural network as a matrix in the same form as Y\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "  assert(Y.shape[1] == Y_pred.shape[1])\n",
        "  M = Y.shape[1]\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](Y_pred, Y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = (grad_a_k_L @ network[k]['cache'][0].T / M) + weight_decay * grad_reglr_fn(network[k]['weights'])\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = np.mean(grad_a_k_L, axis=1)\n",
        "    grad_b_L[k] = grad_b_L[k].reshape((grad_b_L[k].shape[0], 1))\n",
        "    \n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k-1]['cache'][1])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knKqQELwYpeb"
      },
      "source": [
        "def L2_regularisation(network):\n",
        "  # Function that returns L2 regularisation loss for the given network\n",
        "  L = len(network)\n",
        "  ans = 0\n",
        "  for k in range(L):\n",
        "    ans += np.sum(network[k]['weights'] ** 2)\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def L1_regularisation(network):\n",
        "  # Function that returns L1 regularisation loss for the given network\n",
        "  L = len(network)\n",
        "  ans = 0\n",
        "  for k in range(L):\n",
        "    ans += np.sum(np.absolute(network[k]['weights']))\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def grad_L2_regularisation(W):\n",
        "  # Function that returns L2 regularisation gradient for the given Weight matrix / tensor\n",
        "  return 2 * W\n",
        "\n",
        "def grad_L1_regularisation(W):\n",
        "  # Function that returns L1 regularisation gradient for the given Weight matrix / tensor\n",
        "  return np.sign(W)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(Y_pred, Y_true):\n",
        "  # Function that returns cross entropy loss\n",
        "  assert(Y_pred.shape[1] == Y_true.shape[1])\n",
        "  M = Y_pred.shape[1]\n",
        "  return -(Y_true * np.log(Y_pred)).sum() / M\n",
        "\n",
        "def SquaredError(Y_pred, Y_true):\n",
        "  # Function that returns mean squared loss\n",
        "  assert(Y_pred.shape[1] == Y_true.shape[1])\n",
        "  M = Y_pred.shape[1]\n",
        "  return ((Y_true - Y_pred) ** 2).sum() / (2.0 * M)\n",
        "\n",
        "def overall_loss(network, Y_pred, Y_true, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation):\n",
        "  # Function that returns overall loss (Output layer cost function + Regularisation) for the network\n",
        "  return loss_fn(Y_pred, Y_true) + weight_decay * regularisation_fn(network)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6abLgZ7KRtM"
      },
      "source": [
        "  def calc_accuracy_loss(network, X, Y, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation):\n",
        "    # Function that returns the accuracy and loss for a given network\n",
        "    Y_pred = forward_propagation(network, X)\n",
        "    loss = overall_loss(network, Y_pred, Y, loss_fn, weight_decay, regularisation_fn)\n",
        "    assert(X.shape[1] == Y.shape[1])\n",
        "    M = X.shape[1]\n",
        "    accuracy = np.sum(np.argmax(Y_pred, axis = 0) == np.argmax(Y, axis = 0)) / M\n",
        "    return accuracy, loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def sgd_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, weight_decay = 0, \n",
        "                         regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input training data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True training output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "  \n",
        "  for epoch in tqdm(range(max_epochs)):        \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "    \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9, weight_decay = 0, \n",
        "                              regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k]\n",
        "  \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n",
        "\n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9, weight_decay=0, \n",
        "                              regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  lookahead_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred_org = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred_org, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      Y_pred = forward_propagation(lookahead_network, X[:,i:i+batch_size])\n",
        "      dw, db = back_propagation(lookahead_network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k] \n",
        "        lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "        lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    \n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "\n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def rmsprop_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9,\n",
        "                             weight_decay=0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                             validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "    \n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        batch += 1\n",
        "        Y_true = Y[:,i:i+batch_size]\n",
        "        Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "        batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "        dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "        batch_loss_values.append(batch_curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "        for k in range(L):\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "    \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "\n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 50, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                          beta2 = 0.999, weight_decay=0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                          validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  \n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, v_w, v_b, m_w, m_b, = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                                [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                                [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "  \n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        batch += 1\n",
        "        Y_true = Y[:,i:i+batch_size]\n",
        "        Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "        batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "        dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "        batch_loss_values.append(batch_curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "        for k in range(L):\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lk1FkKdZj1"
      },
      "source": [
        "# Nadam optimisation followed from this paper : https://arxiv.org/pdf/1609.04747.pdf\n",
        "def nadam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                           beta2 = 0.999, weight_decay=0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                           validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nadam Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b, v_w, v_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                               [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                               [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    \n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOmXZeotoxbV"
      },
      "source": [
        "def convert_to_onehot(Y, N) :\n",
        "  # Converting output data Y to onehot representation\n",
        "  Y_onehot = []\n",
        "  for y in Y:\n",
        "    curr_y = [0] * N\n",
        "    curr_y[y] = 1\n",
        "    Y_onehot.append(curr_y)\n",
        "\n",
        "  return np.array(Y_onehot).T\n",
        "\n",
        "def train_validation_split(X, Y, train_to_valid_ratio = 0.9):\n",
        "  # Function to split the given input and output data into training and validation sets in the ratio given by train_to_valid_ratio\n",
        "  assert(train_to_valid_ratio > 0 and train_to_valid_ratio < 1)\n",
        "\n",
        "  perm = np.random.permutation(trainX.shape[0])\n",
        "  train_size = int(train_to_valid_ratio * len(perm))\n",
        "  train_indices = perm[:train_size]\n",
        "  valid_indices = perm[train_size:]\n",
        "\n",
        "  return X[train_indices], Y[train_indices], X[valid_indices], Y[valid_indices]\n",
        "\n",
        "def transform_NN_IO(X, Y, no_output_class):\n",
        "  # Function to transform the input and output to a form compatible with the Neural Network functions\n",
        "  X_norm = X.reshape(X.shape[0], (X.shape[1]*X.shape[2])).T / 255   # Input Training data with ith column being ith training example's data\n",
        "  Y_onehot = convert_to_onehot(Y, no_output_class)                  # Converting y labels to onehot representation\n",
        "\n",
        "  return X_norm, Y_onehot\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv"
      },
      "source": [
        "# Encoding the functions with strings for using in sweep\n",
        "get_grad = {\n",
        "    'sigmoid' : grad_sigmoid,\n",
        "    'tanh' : grad_tanh,\n",
        "    'relu' : grad_relu\n",
        "}\n",
        "\n",
        "get_gd_function = {\n",
        "    'sgd' : sgd_gradient_descent, \n",
        "    'momentum' : momentum_gradient_descent, \n",
        "    'nesterov' : nesterov_gradient_descent, \n",
        "    'rmsprop' : rmsprop_gradient_descent, \n",
        "    'adam' : adam_gradient_descent, \n",
        "    'nadam' : nadam_gradient_descent \n",
        "}\n",
        "\n",
        "get_activ_fn = {\n",
        "    'sigmoid' : sigmoid,\n",
        "    'tanh' : tanh,\n",
        "    'relu' : relu\n",
        "}\n",
        "\n",
        "get_weight_init_fn = {\n",
        "    'random': random_initialisation, \n",
        "    'xavier': xavier_initialisation\n",
        "}\n",
        "\n",
        "get_regularisation_fn = {\n",
        "    'L2': L2_regularisation,\n",
        "    'L1': L1_regularisation\n",
        "}\n",
        "\n",
        "get_grad_reglr_fn = {\n",
        "    'L2': grad_L2_regularisation,\n",
        "    'L1': grad_L1_regularisation\n",
        "}\n",
        "\n",
        "\n",
        "def train_NN(trainX, trainY, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn,\n",
        "             activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, \n",
        "             grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation,\n",
        "             grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None, testX = None, testY = None):\n",
        "  \n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"no_epochs\": max_epochs,\n",
        "                      })\n",
        "  \n",
        "  assert(trainX.shape[1] == trainY.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  if testY is not None:\n",
        "    assert(testX.shape[1] == testY.shape[1])\n",
        "    \n",
        "  L = no_hidden_layers+1                                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [size_hidden_layer] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = trainX.shape[0]\n",
        "  n_L[L] = trainY.shape[0]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  batch_loss_values, train_stats, valid_stats = optimisation_fn(trainX, trainY, network, batch_size, learning_rate, max_epochs, \n",
        "                                                                loss_fn, weight_decay = weight_decay, regularisation_fn = regularisation_fn, \n",
        "                                                                grad_reglr_fn = grad_reglr_fn, validX = validX, validY = validY)\n",
        "  if testY is not None: \n",
        "    test_stats = calc_accuracy_loss(network, testX, testY, loss_fn, weight_decay, regularisation_fn)\n",
        "    wandb.log({'test_accuracy': test_stats[0], 'test_loss': test_stats[1]})\n",
        "    print(test_stats)\n",
        "\n",
        "  plt.plot(batch_loss_values)\n",
        "  plt.show()\n",
        "  # plt.plot(train_stats[:][0])\n",
        "  # plt.show()\n",
        "  # plt.plot(train_stats[:][1])\n",
        "  # plt.show()\n",
        "\n",
        "  return network"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dd13f569360b419a8b5e646c2efaff6f",
            "ad14abf4b0094727b4f80b1d3e525c03",
            "d347893b01f94b01924ba44b21f37ff2",
            "848b38c08e194923a05a7ef9f05ce7f9",
            "0b81e30c4c364e63aa8b4d28a904607f",
            "4ca3adb1a208443abc78fce17f85896e",
            "ce19490681f34ce1a6cb4fef3d1c56db",
            "106676763303483a8e5c705abb61cd08",
            "6b5ab8fe81c248b0b9e3b5df324f3d75",
            "d095224745734cf6afc33586ca6238f9",
            "1378e4fd092f48d585e52c945d71cca4",
            "d16f16ff8359445889d2f0c45ed1e385",
            "c07bb1a8480a40f3b695fa3d87a9d606",
            "5aa85d8732094e4fb3dff6a757e8490d",
            "520b895e0c4b4304bad644188722c90f",
            "851c6db52e5544caa15446e3f83ca045"
          ]
        },
        "id": "OKQ4md7QrEuy",
        "outputId": "5da0087f-c4f1-4697-d8cc-ca7e0ca48015"
      },
      "source": [
        "trainX_split, trainY_split, validX_split, validY_split = train_validation_split(trainX, trainy, 0.9)\n",
        "trainX_tr, trainY_tr = transform_NN_IO(trainX_split, trainY_split, N_CLASSES)\n",
        "validX_tr, validY_tr = transform_NN_IO(validX_split, validY_split, N_CLASSES)\n",
        "testX_tr, testY_tr = transform_NN_IO(testX, testy, N_CLASSES)\n",
        "\n",
        "# Questions 2,3\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "network = train_NN(trainX_tr, trainY_tr, sgd_gradient_descent, 16, 1e-2, 10, 2, 10, random_initialisation, tanh, linear, softmax, grad_tanh,\n",
        "                   grad_output_fn = Softmax_SquaredError_grad, loss_fn = SquaredError,\n",
        "                   weight_decay = 0.0005, validX = validX_tr, validY = validY_tr, testX = testX_tr, testY = testY_tr)\n",
        "run.finish()\n",
        "\n",
        "# print(f'Training data original output : {trainy_onehot[0]}')\n",
        "# print(f'Training data output from NN : {forward_propagation(network, trainX_reshaped[0].reshape((trainX_reshaped.shape[1],1))).T}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1m71m5ex) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 123<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd13f569360b419a8b5e646c2efaff6f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210311_102843-1m71m5ex/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210311_102843-1m71m5ex/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">daily-sun-1008</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1m71m5ex\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1m71m5ex</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1m71m5ex). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">pious-sun-1009</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/2f7haacu\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/2f7haacu</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210311_102942-2f7haacu</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:32<00:00,  3.28s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0.7964, 0.20125937010194261)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 163<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b5ab8fe81c248b0b9e3b5df324f3d75",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210311_102942-2f7haacu/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210311_102942-2f7haacu/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>33750</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>batch_loss</td><td>0.10964</td></tr><tr><td>_runtime</td><td>35</td></tr><tr><td>_timestamp</td><td>1615458620</td></tr><tr><td>_step</td><td>33760</td></tr><tr><td>validation_accuracy</td><td>0.80033</td></tr><tr><td>accuracy</td><td>0.80978</td></tr><tr><td>validation_loss</td><td>0.20101</td></tr><tr><td>loss</td><td>0.19523</td></tr><tr><td>test_accuracy</td><td>0.7964</td></tr><tr><td>test_loss</td><td>0.20126</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>batch_loss</td><td>███████▇▇▇▇▆▆▆▆▆▆▄▄▄▃▆▄▃▄▄▄▃▃▃▂▃▄▂▃▃▁▂▃▂</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁▁▃▅▆▇▇▇██</td></tr><tr><td>accuracy</td><td>▁▁▃▅▆▇▇▇██</td></tr><tr><td>validation_loss</td><td>█▇▆▅▃▃▂▂▁▁</td></tr><tr><td>loss</td><td>█▇▆▅▃▃▂▂▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">pious-sun-1009</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/2f7haacu\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/2f7haacu</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'no_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'no_hidden_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'size_hidden_layer': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'weight_decay' :{\n",
        "            'values': [0, 0.005, 0.0005]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-1, 1e-2, 1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam' ]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_initialisation': {\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'activation_fn': {\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity=\"abisheks\", project=\"assignment1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYFa9Jh7rU0"
      },
      "source": [
        "def sweep_wrapper():\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults =  {\n",
        "      'no_epochs': 5,\n",
        "      'no_hidden_layers': 3,\n",
        "      'size_hidden_layer': 32, \n",
        "      'weight_decay' : 0,\n",
        "      'learning_rate': 1e-2,\n",
        "      'optimizer': 'adam',\n",
        "      'batch_size': 128,\n",
        "      'weight_initialisation': 'random', \n",
        "      'activation_fn': 'relu'\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  run = wandb.init(config=config_defaults, reinit=True)\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "  wandb.config.update({'no_classes': N_CLASSES})\n",
        "\n",
        "  wandb.run.name = f'hl_{config.no_hidden_layers}_bs_{config.batch_size}_ac_{config.activation_fn}'\n",
        "  wandb.run.save()\n",
        "\n",
        "  train_NN(trainX_tr, trainY_tr, get_gd_function[config.optimizer], config.batch_size, config.learning_rate, \n",
        "           config.no_epochs, config.no_hidden_layers, config.size_hidden_layer, get_weight_init_fn[config.weight_initialisation],\n",
        "           get_activ_fn[config.activation_fn], linear, softmax, get_grad[config.activation_fn], \n",
        "           Softmax_SquaredError_grad, SquaredError, config.weight_decay, L2_regularisation, grad_L2_regularisation, \n",
        "           validX_tr, validY_tr, testX_tr, testY_tr)\n",
        "  \n",
        "  run.finish()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTMh4OI7HGzC"
      },
      "source": [
        "# Questions 4-6\n",
        "# wandb.agent('abisheks/assignment1/9ks8kwx5', sweep_wrapper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHHZemXkdwVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}