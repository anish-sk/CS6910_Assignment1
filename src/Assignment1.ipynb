{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VectorizeAssignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b95be79613b14dcc9c0830ca52fac283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_723a5989e8234d8197d68853bd9fe877",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c9c1d09f7db4542a504d75f209ba810",
              "IPY_MODEL_662b85f0a70548a6afbf46eab92001dc"
            ]
          }
        },
        "723a5989e8234d8197d68853bd9fe877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c9c1d09f7db4542a504d75f209ba810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_75636a5c063444c8bf9fdba6cf28ec52",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a56f9675bee04d378f588d114e287928"
          }
        },
        "662b85f0a70548a6afbf46eab92001dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b032056a23d04b1db8bceea7325c96cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31ee28b75d364083b61d9ff96ccf6bb2"
          }
        },
        "75636a5c063444c8bf9fdba6cf28ec52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a56f9675bee04d378f588d114e287928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b032056a23d04b1db8bceea7325c96cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31ee28b75d364083b61d9ff96ccf6bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67abe52f69f64a17b0279be614d9ffd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_86fca670d1fe41cba91a4e119169a69d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_246095d4a73b40edbaacf71d87fa561d",
              "IPY_MODEL_dc18de86c33d411b816d8ed797b8914e"
            ]
          }
        },
        "86fca670d1fe41cba91a4e119169a69d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "246095d4a73b40edbaacf71d87fa561d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_65d3d2abe648448d9ea7b48620cb26c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7d4e7c2c3bf418684113bdce313b409"
          }
        },
        "dc18de86c33d411b816d8ed797b8914e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d9ba595c14c429897e142567dcc22c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a779f67e3fa4dba91cd1791d4286679"
          }
        },
        "65d3d2abe648448d9ea7b48620cb26c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7d4e7c2c3bf418684113bdce313b409": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d9ba595c14c429897e142567dcc22c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a779f67e3fa4dba91cd1791d4286679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62880c7-2779-4d90-d271-33ed806a361d"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.7/dist-packages (0.10.22)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied, skipping upgrade: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc51f53-f63b-410b-8f59-2e0debfa95b2"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "\n",
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "# getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)\n",
        "# run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def relu(X):\n",
        "  # Calculates the Rectified Linear Unit (ReLU) function\n",
        "  return np.maximum(X,0)\n",
        "\n",
        "def sigmoid(X):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -X))\n",
        "\n",
        "def softmax(X):\n",
        "  # Calculates the softmax function\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "def tanh(X):\n",
        "  return np.tanh(X)\n",
        "\n",
        "def linear(W, X, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ X + b\n",
        "\n",
        "def grad_relu(X):\n",
        "  # Calculates the gradient of Rectified Linear Unit (ReLU) function\n",
        "  return X > 0\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(X) * (1 - sigmoid(X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(X)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(Y_pred, Y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(Y - Y_pred)\n",
        "\n",
        "def Softmax_SquaredError_grad(Y_pred, Y):\n",
        "  return ((Y_pred - Y) - ((Y_pred - Y) * Y_pred).sum(axis = 0)) * Y_pred "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "  # Initialising a matrix by xavier initialisation with given dimensions (shape) as tuple\n",
        "  bound = (6/(shape[0]+shape[1]))**(0.5)\n",
        "  return bound*(2*np.random.rand(*shape)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "perm = np.random.permutation(trainX.shape[0])\n",
        "train_size = int(0.9* len(perm))\n",
        "train_indices = perm[:train_size]\n",
        "valid_indices = perm[train_size:]\n",
        "trainX_reshaped = trainX.reshape(trainX.shape[0], (trainX.shape[1]*trainX.shape[2])) / 255  # Input Training data with ith column being ith training example's data\n",
        "validX_reshaped = trainX_reshaped[valid_indices]\n",
        "validy = trainy[valid_indices]\n",
        "trainX_reshaped = trainX_reshaped[train_indices]\n",
        "trainy = trainy[train_indices]\n",
        "testX_reshaped = testX.reshape(testX.shape[0], (testX.shape[1]*testX.shape[2])) / 255       # Input Training data with ith column being ith training example's data\n",
        "\n",
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "# wandb.config.update({\"n_hidden_layers\": 3, \"size_hidden_layer\":32})    # Setting the hyperparameters in the wandb\n",
        "# L = wandb.config['n_hidden_layers']+1                                 # Number of hidden layerws + Output layer in the neural network\n",
        "# n_L = [wandb.config['size_hidden_layer']] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "# n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "# n_L[L] = N_CLASSES\n",
        "\n",
        "# pre_act_fns_L = [linear] * L                  # List of Pre-activation functions of the hidden layers and output layer\n",
        "# act_fns_L = [sigmoid] * (L-1) + [softmax]     # List of Activation functions of the hidden layers and output layer\n",
        "# grad_act_fns_L = [grad_sigmoid] * (L-1)       # List of Gradients of the Activation functions, of the hidden layers\n",
        "# grad_output_fn = Softmax_CrossEntropy_grad\n",
        "\n",
        "# network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, random_initialisation)\n",
        "# print(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = H_prev                    # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  cache = (pre_activation_cache, A)         # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, X):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      X -- Input data from the training set as a matrix with ith column representing ith input training data\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network as a matrix with ith column representing output of ith input data\n",
        "    \"\"\"\n",
        "\n",
        "    H = X                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "\n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "        \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, Y, Y_pred, weight_decay, grad_reglr_fn):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      Y -- True output matrix with ith column representing true output of ith input data\n",
        "      Y_pred -- Output of neural network as a matrix in the same form as Y\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "  assert(Y.shape[1] == Y_pred.shape[1])\n",
        "  M = Y.shape[1]\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](Y_pred, Y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = (grad_a_k_L @ network[k]['cache'][0].T / M) + weight_decay * grad_reglr_fn(network[k]['weights'])\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = np.mean(grad_a_k_L, axis=1)\n",
        "    grad_b_L[k] = grad_b_L[k].reshape((grad_b_L[k].shape[0], 1))\n",
        "    \n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k-1]['cache'][1])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knKqQELwYpeb"
      },
      "source": [
        "def L2_regularisation(network):\n",
        "  L = len(network)\n",
        "  ans = 0\n",
        "  for k in range(L):\n",
        "    ans += np.sum(network[k]['weights'] ** 2)\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def L1_regularisation(network):\n",
        "  L = len(network)\n",
        "  ans = 0\n",
        "  for k in range(L):\n",
        "    ans += np.sum(np.absolute(network[k]['weights']))\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def grad_L2_regularisation(W):\n",
        "  return 2 * W\n",
        "\n",
        "def grad_L1_regularisation(W):\n",
        "  return np.sign(W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(Y_pred, Y_true):\n",
        "  assert(Y_pred.shape[1] == Y_true.shape[1])\n",
        "  M = Y_pred.shape[1]\n",
        "  return -(Y_true * np.log(Y_pred)).sum() / M\n",
        "\n",
        "def SquaredError(Y_pred, Y_true):\n",
        "  assert(Y_pred.shape[1] == Y_true.shape[1])\n",
        "  M = Y_pred.shape[1]\n",
        "  return ((Y_true - Y_pred) ** 2).sum() / (2.0 * M)\n",
        "\n",
        "def overall_loss(network, Y_pred, Y_true, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation):\n",
        "  return loss_fn(Y_pred, Y_true) + weight_decay * regularisation_fn(network)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6abLgZ7KRtM"
      },
      "source": [
        "  def calc_accuracy_loss(network, X, Y, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation):\n",
        "    Y_pred = forward_propagation(network, X)\n",
        "    loss = overall_loss(network, Y_pred, Y, loss_fn, weight_decay, regularisation_fn)\n",
        "    assert(X.shape[1] == Y.shape[1])\n",
        "    M = X.shape[1]\n",
        "    accuracy = np.sum(np.argmax(Y_pred, axis = 0) == np.argmax(Y, axis = 0)) / M\n",
        "    return accuracy, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def sgd_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, weight_decay = 0, \n",
        "                         regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input training data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True training output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "  \n",
        "  for epoch in tqdm(range(max_epochs)):        \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "    \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9, weight_decay = 0, \n",
        "                              regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k]\n",
        "  \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n",
        "\n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9, weight_decay=0, \n",
        "                              regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  lookahead_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred_org = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred_org, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      Y_pred = forward_propagation(lookahead_network, X[:,i:i+batch_size])\n",
        "      dw, db = back_propagation(lookahead_network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k] \n",
        "        lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "        lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    \n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "\n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def rmsprop_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9,\n",
        "                             weight_decay=0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                             validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "    \n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        batch += 1\n",
        "        Y_true = Y[:,i:i+batch_size]\n",
        "        Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "        batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "        dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "        batch_loss_values.append(batch_curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "        for k in range(L):\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "    \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "\n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 50, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                          beta2 = 0.999, weight_decay=0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                          validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  \n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b, m_w, m_b, = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                                [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                                [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "  \n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        batch += 1\n",
        "        Y_true = Y[:,i:i+batch_size]\n",
        "        y_pred = forward_propagation(network, x)\n",
        "        batch_curr_loss += overall_loss(network, y_pred, y, loss_fn, weight_decay, regularisation_fn)\n",
        "        grad_w_L, grad_b_L = back_propagation(network, y, y_pred, weight_decay, grad_reglr_fn)\n",
        "        batch_loss_values.append(batch_curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "        for k in range(L):\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lk1FkKdZj1"
      },
      "source": [
        "# Nadam optimisation followed from this paper : https://arxiv.org/pdf/1609.04747.pdf\n",
        "def nadam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                           beta2 = 0.999, weight_decay=0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                           validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nadam Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (accuracy, average loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b, v_w, v_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                               [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                               [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    \n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b95be79613b14dcc9c0830ca52fac283",
            "723a5989e8234d8197d68853bd9fe877",
            "5c9c1d09f7db4542a504d75f209ba810",
            "662b85f0a70548a6afbf46eab92001dc",
            "75636a5c063444c8bf9fdba6cf28ec52",
            "a56f9675bee04d378f588d114e287928",
            "b032056a23d04b1db8bceea7325c96cb",
            "31ee28b75d364083b61d9ff96ccf6bb2",
            "67abe52f69f64a17b0279be614d9ffd1",
            "86fca670d1fe41cba91a4e119169a69d",
            "246095d4a73b40edbaacf71d87fa561d",
            "dc18de86c33d411b816d8ed797b8914e",
            "65d3d2abe648448d9ea7b48620cb26c6",
            "d7d4e7c2c3bf418684113bdce313b409",
            "6d9ba595c14c429897e142567dcc22c4",
            "7a779f67e3fa4dba91cd1791d4286679"
          ]
        },
        "outputId": "262b9a1f-4b08-49bb-8240-d9286bd742b2"
      },
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "get_grad = {\n",
        "    'sigmoid' : grad_sigmoid,\n",
        "    'tanh' : grad_tanh,\n",
        "    'relu' : grad_relu\n",
        "}\n",
        "\n",
        "get_gd_function = {\n",
        "    'sgd' : sgd_gradient_descent, \n",
        "    'momentum' : momentum_gradient_descent, \n",
        "    'nesterov' : nesterov_gradient_descent, \n",
        "    'rmsprop' : rmsprop_gradient_descent, \n",
        "    'adam' : adam_gradient_descent, \n",
        "    'nadam' : nadam_gradient_descent \n",
        "}\n",
        "\n",
        "get_activ_fn = {\n",
        "    'sigmoid' : sigmoid,\n",
        "    'tanh' : tanh,\n",
        "    'relu' : relu\n",
        "}\n",
        "\n",
        "get_weight_init_fn = {\n",
        "    'random': random_initialisation, \n",
        "    'xavier': xavier_initialisation\n",
        "}\n",
        "\n",
        "get_regularisation_fn = {\n",
        "    'L2': L2_regularisation,\n",
        "    'L1': L1_regularisation\n",
        "}\n",
        "\n",
        "get_grad_reglr_fn = {\n",
        "    'L2': grad_L2_regularisation,\n",
        "    'L1': grad_L1_regularisation\n",
        "}\n",
        "\n",
        "\n",
        "def convert_to_onehot(Y, N) :\n",
        "  # Converting output data Y to onehot representation\n",
        "  Y_onehot = []\n",
        "  for y in Y:\n",
        "    curr_y = [0] * N\n",
        "    curr_y[y] = 1\n",
        "    Y_onehot.append(curr_y)\n",
        "\n",
        "  return np.array(Y_onehot).T\n",
        "\n",
        "\n",
        "def train_NN(trainX, trainY, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn,\n",
        "             activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, \n",
        "             grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation,\n",
        "             grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None, testX = None, testY = None):\n",
        "  \n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"no_epochs\": max_epochs,\n",
        "                      })\n",
        "  \n",
        "  assert(trainX.shape[0] == trainY.shape[0])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[0] == validY.shape[0])\n",
        "  if testY is not None:\n",
        "    assert(testX.shape[0] == testY.shape[0])\n",
        "\n",
        "  # Converting y labels to onehot representation for training, validation and testing\n",
        "  trainY_onehot = convert_to_onehot(trainY, N_CLASSES)\n",
        "  trainX = trainX.T\n",
        "  validY_onehot = None\n",
        "  if validY is not None:\n",
        "    validY_onehot = convert_to_onehot(validY, N_CLASSES)\n",
        "    validX = validX.T\n",
        "  testY_onehot = None\n",
        "  if testY is not None:\n",
        "    testY_onehot = convert_to_onehot(testY, N_CLASSES)\n",
        "    testX = testX.T\n",
        "  \n",
        "  print(trainX.shape, trainY_onehot.shape)\n",
        "    \n",
        "  L = no_hidden_layers+1                                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [size_hidden_layer] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = trainX.shape[0]\n",
        "  n_L[L] = trainY_onehot.shape[0]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  batch_loss_values, train_stats, valid_stats = optimisation_fn(trainX, trainY_onehot, network, batch_size, learning_rate, max_epochs, \n",
        "                                                                loss_fn, weight_decay = weight_decay, regularisation_fn = regularisation_fn, \n",
        "                                                                grad_reglr_fn = grad_reglr_fn, validX = validX, validY = validY_onehot)\n",
        "  if testY is not None: \n",
        "    test_stats = calc_accuracy_loss(network, testX, testY_onehot, loss_fn, weight_decay, regularisation_fn)\n",
        "    wandb.log({'test_accuracy': test_stats[0], 'test_loss': test_stats[1]})\n",
        "    print(test_stats)\n",
        "\n",
        "  plt.plot(batch_loss_values)\n",
        "  plt.show()\n",
        "  # plt.plot(train_stats[:][0])\n",
        "  # plt.show()\n",
        "  # plt.plot(train_stats[:][1])\n",
        "  # plt.show()\n",
        "\n",
        "  return network\n",
        "\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "network = train_NN(trainX_reshaped, trainy, sgd_gradient_descent, 16, 1e-2, 10, 2, 10, random_initialisation, tanh, linear, softmax, grad_tanh,\n",
        "                   grad_output_fn = Softmax_SquaredError_grad, loss_fn = SquaredError,\n",
        "                   weight_decay = 0.0005, validX = validX_reshaped, validY = validy, testX = testX_reshaped, testY = testy)\n",
        "run.finish()\n",
        "\n",
        "# print(f'Training data original output : {trainy_onehot[0]}')\n",
        "# print(f'Training data output from NN : {forward_propagation(network, trainX_reshaped[0].reshape((trainX_reshaped.shape[1],1))).T}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1nfuhy3x) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2357<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b95be79613b14dcc9c0830ca52fac283",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210310_152825-1nfuhy3x/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210310_152825-1nfuhy3x/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">ancient-terrain-442</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1nfuhy3x\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1nfuhy3x</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1nfuhy3x). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dry-serenity-443</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/pfrd392a\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/pfrd392a</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210310_152938-pfrd392a</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(784, 54000) (10, 54000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 1/10 [00:03<00:32,  3.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 2/10 [00:07<00:28,  3.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 3/10 [00:10<00:25,  3.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 4/10 [00:14<00:21,  3.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 50%|█████     | 5/10 [00:17<00:17,  3.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 6/10 [00:21<00:14,  3.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 70%|███████   | 7/10 [00:24<00:10,  3.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 8/10 [00:28<00:07,  3.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 90%|█████████ | 9/10 [00:32<00:03,  3.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 10/10 [00:35<00:00,  3.56s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0.7778, 0.21015190935340017)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 2399<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67abe52f69f64a17b0279be614d9ffd1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210310_152938-pfrd392a/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210310_152938-pfrd392a/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>33750</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>batch_loss</td><td>0.21459</td></tr><tr><td>_runtime</td><td>38</td></tr><tr><td>_timestamp</td><td>1615390219</td></tr><tr><td>_step</td><td>33760</td></tr><tr><td>validation_accuracy</td><td>0.788</td></tr><tr><td>accuracy</td><td>0.78843</td></tr><tr><td>validation_loss</td><td>0.20642</td></tr><tr><td>loss</td><td>0.20578</td></tr><tr><td>test_accuracy</td><td>0.7778</td></tr><tr><td>test_loss</td><td>0.21015</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>batch_loss</td><td>███████▇▇▆▆▆▆▆▆▅▅▄▅▄▄▄▃▄▄▂▃▃▃▂▂▄▃▃▃▃▁▂▃▃</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁▂▃▅▆▇▇▇██</td></tr><tr><td>accuracy</td><td>▁▂▃▅▆▇▇▇██</td></tr><tr><td>validation_loss</td><td>█▇▆▅▃▂▂▂▁▁</td></tr><tr><td>loss</td><td>█▇▆▅▃▃▂▂▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">dry-serenity-443</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/pfrd392a\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/pfrd392a</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',                   # Possible search : grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'no_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'no_hidden_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'size_hidden_layer': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'weight_decay' :{\n",
        "            'values': [0, 0.005, 0.0005]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-1, 1e-2, 1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam' ]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_initialisation': {\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'activation_fn': {\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity=\"abisheks\", project=\"assignment1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYFa9Jh7rU0"
      },
      "source": [
        "def sweep_wrapper():\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults =  {\n",
        "      'no_epochs': 5,\n",
        "      'no_hidden_layers': 3,\n",
        "      'size_hidden_layer': 32, \n",
        "      'weight_decay' : 0,\n",
        "      'learning_rate': 1e-2,\n",
        "      'optimizer': 'adam',\n",
        "      'batch_size': 128,\n",
        "      'weight_initialisation': 'random', \n",
        "      'activation_fn': 'relu'\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  run = wandb.init(config=config_defaults, reinit=True)\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "  wandb.config.update({'no_classes': N_CLASSES})\n",
        "\n",
        "  train_NN(trainX_reshaped, trainy, get_gd_function[config.optimizer], config.batch_size, config.learning_rate, \n",
        "           config.no_epochs, config.no_hidden_layers, config.size_hidden_layer, get_weight_init_fn[config.weight_initialisation],\n",
        "           get_activ_fn[config.activation_fn], linear, softmax, get_grad[config.activation_fn], \n",
        "           Softmax_CrossEntropy_grad, CrossEntropy_loss, config.weight_decay, L2_regularisation, grad_L2_regularisation, \n",
        "           validX_reshaped, validy)\n",
        "  \n",
        "  run.finish()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTMh4OI7HGzC"
      },
      "source": [
        "# wandb.agent('abisheks/assignment1/cfj2slke', sweep_wrapper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHHZemXkdwVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}