{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "764fb6bbd4784e70a77dd9007ba3e2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_089e9502bc3b4f76bca502944a6c0ae4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3ad942e6cb4d43ee8985f50418cc7c65",
              "IPY_MODEL_cc4d419885c7481e9a66cadc65fa7080"
            ]
          }
        },
        "089e9502bc3b4f76bca502944a6c0ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ad942e6cb4d43ee8985f50418cc7c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_22906640c0074c8f88ab324b24c47b9e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1d79764feaf4f2485f116f56a447666"
          }
        },
        "cc4d419885c7481e9a66cadc65fa7080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02a596b6ffe44b51968d3e5f3b4f3fe2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ec8ab634b9945baa8898d5c5978fecf"
          }
        },
        "22906640c0074c8f88ab324b24c47b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1d79764feaf4f2485f116f56a447666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "02a596b6ffe44b51968d3e5f3b4f3fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ec8ab634b9945baa8898d5c5978fecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdeec97351a94dcd921f233dd0988296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c513b6357bf34cc58dda6513066fed1a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aedd6247b5c34e3a867ef1d403d7b04f",
              "IPY_MODEL_ea953aae17f842a5ac976bb7ba2e5cc0"
            ]
          }
        },
        "c513b6357bf34cc58dda6513066fed1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aedd6247b5c34e3a867ef1d403d7b04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_1803117e9b13417b947346b01d2f6ae9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ee3b03b44c8411bb978c62afda507a2"
          }
        },
        "ea953aae17f842a5ac976bb7ba2e5cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ab9b7d237484bb0a6dabdbe1d550654",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc20fddd557d4e4cbf0a8dba625a742b"
          }
        },
        "1803117e9b13417b947346b01d2f6ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ee3b03b44c8411bb978c62afda507a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ab9b7d237484bb0a6dabdbe1d550654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc20fddd557d4e4cbf0a8dba625a742b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f434aa8-1b22-4647-aed0-80bce9c08d5c"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.8MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.6MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=fceb098eed968353fb5eb69b8aaa38df6f0a7c98d32e463548f275342c2a670e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=e0b9e49cb64924270489c2d3af8924d3b44601f6d388fabdbab9b1f367e079b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, shortuuid, sentry-sdk, pathtools, docker-pycreds, configparser, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32be9d7-30a4-4478-b51e-10a8201d2a8f"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "# getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)\n",
        "# run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def relu(x):\n",
        "  # Calculates the Rectified Linear Unit (ReLU) function\n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def softmax(x):\n",
        "  # Calculates the softmax function\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def linear(W, x, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ x + b\n",
        "\n",
        "def grad_relu(x):\n",
        "  # Calculates the gradient of Rectified Linear Unit (ReLU) function\n",
        "  return x > 0\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def grad_tanh(x):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(y_pred, y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(y - y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "  # Initialising a matrix by xavier initialisation with given dimensions (shape) as tuple\n",
        "  bound = (6/(shape[0]+shape[1]))**(0.5)\n",
        "  return bound*(2*np.random.rand(*shape)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "perm = np.random.permutation(trainX.shape[0])\n",
        "train_size = int(0.9* len(perm))\n",
        "train_indices = perm[:train_size]\n",
        "valid_indices = perm[train_size:]\n",
        "trainX_reshaped = trainX.reshape(trainX.shape[0], (trainX.shape[1]*trainX.shape[2])) / 255  # Input Training data with ith column being ith training example's data\n",
        "validX_reshaped = trainX_reshaped[valid_indices]\n",
        "validy = trainy[valid_indices]\n",
        "trainX_reshaped = trainX_reshaped[train_indices]\n",
        "trainy = trainy[train_indices]\n",
        "testX_reshaped = testX.reshape(testX.shape[0], (testX.shape[1]*testX.shape[2])) / 255       # Input Training data with ith column being ith training example's data\n",
        "\n",
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "# wandb.config.update({\"n_hidden_layers\": 3, \"size_hidden_layer\":32})    # Setting the hyperparameters in the wandb\n",
        "# L = wandb.config['n_hidden_layers']+1                                 # Number of hidden layerws + Output layer in the neural network\n",
        "# n_L = [wandb.config['size_hidden_layer']] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "# n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "# n_L[L] = N_CLASSES\n",
        "\n",
        "# pre_act_fns_L = [linear] * L                  # List of Pre-activation functions of the hidden layers and output layer\n",
        "# act_fns_L = [sigmoid] * (L-1) + [softmax]     # List of Activation functions of the hidden layers and output layer\n",
        "# grad_act_fns_L = [grad_sigmoid] * (L-1)       # List of Gradients of the Activation functions, of the hidden layers\n",
        "# grad_output_fn = Softmax_CrossEntropy_grad\n",
        "\n",
        "# network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, random_initialisation)\n",
        "# print(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = A   # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  H_prev = H_prev.reshape((H_prev.shape[0], 1))\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  H = H.reshape((H.shape[0],1))\n",
        "  cache = (pre_activation_cache, H_prev)   # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, x):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      x -- Input data from the training set\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    H = x                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "    cache_prev = (x,x) \n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        #network[l]['cache'] = cache_prev\n",
        "        network[l]['cache'] = cache\n",
        "        cache_prev = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "        \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, y, y_pred, weight_decay):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      y -- True output corresponding to the training data input\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](y_pred, y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = grad_a_k_L @ network[k]['cache'][1].T + weight_decay*network[k]['weights']\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = grad_a_k_L\n",
        "\n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k-1]['cache'][0])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(y_pred, y_true):\n",
        "  return -(y_true * np.log(y_pred)).sum()\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "  return ((y_true - y_pred) ** 2).sum() / 2.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def sgd_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, weight_decay=0):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db = [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred, weight_decay)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          network[k]['weights'] -= eta * dw[k]\n",
        "          network[k]['biases'] -= eta * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "  \n",
        "  return loss_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9, \n",
        "                              weight_decay=0):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred, weight_decay)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  return loss_values\n",
        "  \n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9, \n",
        "                              weight_decay=0):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  lookahead_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred_org = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred_org, y)\n",
        "      y_pred = forward_propagation(lookahead_network, x)\n",
        "      grad_w_L, grad_b_L = back_propagation(lookahead_network, y, y_pred, weight_decay)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k] \n",
        "          lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "          lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k]\n",
        "        lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "        lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  return loss_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def rmsprop_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9,\n",
        "                             weight_decay=0):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred, weight_decay)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  return loss_values\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 50, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                          beta2 = 0.999, weight_decay=0):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred, weight_decay)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  return loss_values\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lk1FkKdZj1"
      },
      "source": [
        "# Nadam optimisation followed from this paper : https://arxiv.org/pdf/1609.04747.pdf\n",
        "def nadam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                           beta2 = 0.999, weight_decay=0):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nadam Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred,weight_decay)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  return loss_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "get_grad = {\n",
        "    'sigmoid' : grad_sigmoid,\n",
        "    'tanh' : grad_tanh,\n",
        "    'relu' : grad_relu\n",
        "}\n",
        "\n",
        "get_gd_function = {\n",
        "    'sgd_gradient_descent' : sgd_gradient_descent, \n",
        "    'momentum_gradient_descent' : momentum_gradient_descent, \n",
        "    'nesterov_gradient_descent' : nesterov_gradient_descent, \n",
        "    'rmsprop_gradient_descent' : rmsprop_gradient_descent, \n",
        "    'adam_gradient_descent' : adam_gradient_descent, \n",
        "    'nadam_gradient_descent' : nadam_gradient_descent \n",
        "}\n",
        "\n",
        "get_activ_fn = {\n",
        "    'sigmoid' : sigmoid,\n",
        "    'tanh' : tanh,\n",
        "    'relu' : relu\n",
        "}\n",
        "\n",
        "get_weight_init_fn = {\n",
        "    'random_initialisation':random_initialisation, \n",
        "    'xavier_initialisation':xavier_initialisation\n",
        "}\n",
        "\n",
        "def calc_accuracy(network, testX, testY):\n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX, testY):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "  \n",
        "  acc = n_correct/testX.shape[0]\n",
        "  return acc\n",
        "\n",
        "\n",
        "def train_NN(trainX, trainY, validX, validY, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn,\n",
        "             activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, \n",
        "             grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss, weight_decay = 0):\n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"max_epochs\": max_epochs,\n",
        "                      })\n",
        "  \n",
        "  # Converting y labels to onehot representation for training\n",
        "  trainY_onehot = []\n",
        "  for y in trainY:\n",
        "    curr_y = [0]*N_CLASSES\n",
        "    curr_y[y] = 1\n",
        "    trainY_onehot.append(curr_y)\n",
        "\n",
        "  trainY_onehot = np.array(trainY_onehot)\n",
        "    \n",
        "  L = no_hidden_layers+1                                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [size_hidden_layer] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = trainX.shape[1]\n",
        "  n_L[L] = trainY_onehot.shape[1]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  loss_values = optimisation_fn(trainX, trainY_onehot, network, batch_size, learning_rate, max_epochs, loss_fn)\n",
        "\n",
        "  plt.plot(loss_values)\n",
        "  plt.show()  \n",
        "  valid_acc = calc_accuracy(network, validX, validY)\n",
        "  wandb.log({'validation_accuracy': valid_acc})\n",
        "\n",
        "  return network\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "# network = train_NN(trainX_reshaped[:5], trainy[:5], validX_reshaped, validy, rmsprop_gradient_descent, 128, 1e-3, 20, 2, 10, random_initialisation, tanh,  linear, softmax, grad_tanh)\n",
        "# test_acc = calc_accuracy(network, testX_reshaped, testy)\n",
        "# run.log({'test_accuracy': test_acc})\n",
        "# run.finish()\n",
        "\n",
        "# print(f'Training data original output : {trainy_onehot[0]}')\n",
        "# print(f'Training data output from NN : {forward_propagation(network, trainX_reshaped[0].reshape((trainX_reshaped.shape[1],1))).T}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3546a40-86c8-438b-97be-40d1a104a5e4"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', #grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'max_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'no_hidden_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'size_hidden_layer': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'weight_decay' :{\n",
        "            'values': [0, 0.0005, 0.5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-2, 1e-3, 1e-4]\n",
        "        },\n",
        "        'optimisation_fn': {\n",
        "            'values': ['sgd_gradient_descent', 'momentum_gradient_descent', 'nesterov_gradient_descent', 'rmsprop_gradient_descent', \n",
        "                       'adam_gradient_descent', 'nadam_gradient_descent' ]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_initialisation_fn': {\n",
        "            'values': ['random_initialisation', 'xavier_initialisation']\n",
        "        },\n",
        "        'activation_fn': {\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"abisheks\", project=\"assignment1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 4ia43v08\n",
            "Sweep URL: https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYFa9Jh7rU0"
      },
      "source": [
        "def sweep_wrapper():\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults =  {\n",
        "      'max_epochs': 5,\n",
        "      'no_hidden_layers': 3,\n",
        "      'size_hidden_layer': 32, \n",
        "      'weight_decay' : 0,\n",
        "      'learning_rate': 1e-2,\n",
        "      'optimisation_fn': 'sgd_gradient_descent',\n",
        "      'batch_size': 16,\n",
        "      'weight_initialisation_fn': 'random_initialisation', \n",
        "      'activation_fn': 'relu'\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  run = wandb.init(config=config_defaults, reinit=True)\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "\n",
        "  train_NN(trainX_reshaped, trainy, validX_reshaped, validy, get_gd_function[config.optimisation_fn], config.batch_size, config.learning_rate, \n",
        "           config.max_epochs, config.no_hidden_layers, config.size_hidden_layer, get_weight_init_fn[config.weight_initialisation_fn],\n",
        "           get_activ_fn[config.activation_fn], linear, softmax, get_grad[config.activation_fn], \n",
        "           Softmax_CrossEntropy_grad, CrossEntropy_loss, config.weight_decay)\n",
        "  \n",
        "  run.finish()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTMh4OI7HGzC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "764fb6bbd4784e70a77dd9007ba3e2c3",
            "089e9502bc3b4f76bca502944a6c0ae4",
            "3ad942e6cb4d43ee8985f50418cc7c65",
            "cc4d419885c7481e9a66cadc65fa7080",
            "22906640c0074c8f88ab324b24c47b9e",
            "d1d79764feaf4f2485f116f56a447666",
            "02a596b6ffe44b51968d3e5f3b4f3fe2",
            "7ec8ab634b9945baa8898d5c5978fecf",
            "cdeec97351a94dcd921f233dd0988296",
            "c513b6357bf34cc58dda6513066fed1a",
            "aedd6247b5c34e3a867ef1d403d7b04f",
            "ea953aae17f842a5ac976bb7ba2e5cc0",
            "1803117e9b13417b947346b01d2f6ae9",
            "9ee3b03b44c8411bb978c62afda507a2",
            "7ab9b7d237484bb0a6dabdbe1d550654",
            "fc20fddd557d4e4cbf0a8dba625a742b"
          ]
        },
        "outputId": "c658ae04-df09-4a99-d7bc-9d3ccb612f67"
      },
      "source": [
        "wandb.agent(sweep_id, sweep_wrapper)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: miumx7ls with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_hidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimisation_fn: sgd_gradient_descent\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_hidden_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialisation_fn: xavier_initialisation\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">twilight-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/miumx7ls\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/miumx7ls</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210308_190637-miumx7ls</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_hidden_layers' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'size_hidden_layer' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "100%|██████████| 5/5 [10:05<00:00, 121.09s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 514<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "764fb6bbd4784e70a77dd9007ba3e2c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210308_190637-miumx7ls/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210308_190637-miumx7ls/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>8440</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.5606</td></tr><tr><td>_runtime</td><td>611</td></tr><tr><td>_timestamp</td><td>1615231008</td></tr><tr><td>_step</td><td>8440</td></tr><tr><td>validation_accuracy</td><td>0.79833</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████</td></tr><tr><td>loss</td><td>████▇▆▆▄▄▄▄▃▃▃▄▃▃▂▃▃▂▂▂▂▂▂▂▁▂▁▂▃▁▃▁▁▂▂▂▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">twilight-sweep-1</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/miumx7ls\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/miumx7ls</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o1q9fj7w with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_hidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimisation_fn: sgd_gradient_descent\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_hidden_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialisation_fn: xavier_initialisation\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">cool-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/o1q9fj7w\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/o1q9fj7w</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210308_191654-o1q9fj7w</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_hidden_layers' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'size_hidden_layer' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            "100%|██████████| 5/5 [10:35<00:00, 127.09s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 554<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdeec97351a94dcd921f233dd0988296",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210308_191654-o1q9fj7w/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210308_191654-o1q9fj7w/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>16875</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.27633</td></tr><tr><td>_runtime</td><td>641</td></tr><tr><td>_timestamp</td><td>1615231655</td></tr><tr><td>_step</td><td>16875</td></tr><tr><td>validation_accuracy</td><td>0.86617</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████</td></tr><tr><td>loss</td><td>██▄▄▅▃▃▃▃▂▃▂▂▂▂▁▃▃▂▃▃▄▅▄▄▄▃▄▂▃▁▂▄▅▁▁▂▃▂▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">cool-sweep-2</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/o1q9fj7w\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/o1q9fj7w</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l74x250c with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tno_hidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimisation_fn: nadam_gradient_descent\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_hidden_layer: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialisation_fn: xavier_initialisation\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">expert-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/sweeps/4ia43v08</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/l74x250c\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/l74x250c</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210308_192741-l74x250c</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'no_hidden_layers' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'size_hidden_layer' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'batch_size' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_epochs' was locked by 'sweep' (ignored update).\n",
            " 80%|████████  | 4/5 [03:45<00:56, 56.47s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHHZemXkdwVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}