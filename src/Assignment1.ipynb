{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VectorizeAssignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2314a71b1f1441d790b0c8656f717fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_86e08516f6cf45bb92db05bf75d16ea4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_08f0ea3dddd24aa58d053e1a036a46fc",
              "IPY_MODEL_91d1286314db4acc90b93b902e839829"
            ]
          }
        },
        "86e08516f6cf45bb92db05bf75d16ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08f0ea3dddd24aa58d053e1a036a46fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_a4bf0d072de644fa90a2d982c45c0620",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c44ef1432fc473ab1a21cc4412bab37"
          }
        },
        "91d1286314db4acc90b93b902e839829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0ba7f56b59184163a5033e7982b222bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8c50745a0cbe4a15b9a3c0113e329fac"
          }
        },
        "a4bf0d072de644fa90a2d982c45c0620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c44ef1432fc473ab1a21cc4412bab37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ba7f56b59184163a5033e7982b222bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8c50745a0cbe4a15b9a3c0113e329fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ffc15169cc6245bb849505b3b329aa0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6eaa1156304425e916caf15c2594bcf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b5cd152ba5145bd8e5144a0216cb284",
              "IPY_MODEL_b58a4730c25547e0a8ca0a4d9e084549"
            ]
          }
        },
        "b6eaa1156304425e916caf15c2594bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b5cd152ba5145bd8e5144a0216cb284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_e8a594f4e67044818f508527a7ff0d1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07f18ef686cc4e959e2fedd01a39f06f"
          }
        },
        "b58a4730c25547e0a8ca0a4d9e084549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69c2c7ec66ba4bb8ab5818ef323e676e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06e8ad8e13334a9497afff699e47c8b6"
          }
        },
        "e8a594f4e67044818f508527a7ff0d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07f18ef686cc4e959e2fedd01a39f06f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69c2c7ec66ba4bb8ab5818ef323e676e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06e8ad8e13334a9497afff699e47c8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5aa4039fadd140f9953b3d04e2d11b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_17f7e28481904bc2bbc2b304f435e91c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4421ceae205f4263a4f6ede77e0e96c3",
              "IPY_MODEL_25e2f89caffd437898744f74731dc446"
            ]
          }
        },
        "17f7e28481904bc2bbc2b304f435e91c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4421ceae205f4263a4f6ede77e0e96c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_7331485abfa34577b249534fa3bf47fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_709e8094010449de8c706be9208bfc42"
          }
        },
        "25e2f89caffd437898744f74731dc446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c05a7836d1324718a0440075696ab76a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da2d927496674483a66b79b1a73e34ff"
          }
        },
        "7331485abfa34577b249534fa3bf47fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "709e8094010449de8c706be9208bfc42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c05a7836d1324718a0440075696ab76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da2d927496674483a66b79b1a73e34ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7b8a36-c99a-4c61-9134-14b0b2434047"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 21.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.2MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.8MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=cfc5cc5e75a23850792ff1ea2c66cee0335ca8152d4ad7891a27178f22ff47c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=861653429bdaf75c99b10a6d66cb8175da961bdaf1368c180d6f68e315b7dfa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: docker-pycreds, subprocess32, configparser, shortuuid, sentry-sdk, smmap, gitdb, GitPython, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6744ba2-3125-4875-c6b6-7c9e9f7a1124"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion-MNIST dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "\n",
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take sample image(s) from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(nSamples):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 -- Showing 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "# getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 1)\n",
        "# run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def relu(X):\n",
        "  # Calculates the Rectified Linear Unit (ReLU) function\n",
        "  return np.maximum(X,0)\n",
        "\n",
        "def sigmoid(X):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -X))\n",
        "\n",
        "def softmax(X):\n",
        "  # Calculates the softmax function\n",
        "  e_X = np.exp(X - np.max(X, axis = 0))\n",
        "  return e_X / e_X.sum(axis = 0)\n",
        "\n",
        "def tanh(X):\n",
        "  return np.tanh(X)\n",
        "\n",
        "def linear(W, X, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ X + b\n",
        "\n",
        "def grad_relu(X):\n",
        "  # Calculates the gradient of Rectified Linear Unit (ReLU) function\n",
        "  return X > 0\n",
        "\n",
        "def grad_sigmoid(X):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(X) * (1 - sigmoid(X))\n",
        "\n",
        "def grad_tanh(X):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(X)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(Y_pred, Y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(Y - Y_pred)\n",
        "\n",
        "def Softmax_SquaredError_grad(Y_pred, Y):\n",
        "  return ((Y_pred - Y) - ((Y_pred - Y) * Y_pred).sum(axis = 0)) * Y_pred "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising a random matrix with given dimensions (shape) as tuple\n",
        "  np.random.seed(0)\n",
        "  return np.random.randn(*shape)*0.1\n",
        "\n",
        "def xavier_initialisation(shape):\n",
        "  # Initialising a matrix by xavier initialisation with given dimensions (shape) as tuple\n",
        "  np.random.seed(0)\n",
        "  bound = (6/(shape[0]+shape[1]))**(0.5)\n",
        "  return bound*(2*np.random.rand(*shape)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "####### We have used fully vectorised implementations for the feedforward and backpropogation algorithms to make the training much faster ########\n",
        "\n",
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = H_prev                    # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  cache = (pre_activation_cache, A)         # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, X):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      X -- Input data from the training set as a matrix with ith column representing ith input training data\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network as a matrix with ith column representing output of ith input data\n",
        "    \"\"\"\n",
        "\n",
        "    H = X                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "\n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "        \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, Y, Y_pred, weight_decay, grad_reglr_fn):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      Y -- True output matrix with ith column representing true output of ith input data\n",
        "      Y_pred -- Output of neural network as a matrix in the same form as Y\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "  assert(Y.shape[1] == Y_pred.shape[1])\n",
        "  M = Y.shape[1]\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](Y_pred, Y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = (grad_a_k_L @ network[k]['cache'][0].T / M) + weight_decay * grad_reglr_fn(network[k]['weights'])\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = np.mean(grad_a_k_L, axis=1)\n",
        "    grad_b_L[k] = grad_b_L[k].reshape((grad_b_L[k].shape[0], 1))\n",
        "    \n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k-1]['cache'][1])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knKqQELwYpeb"
      },
      "source": [
        "def L2_regularisation(network):\n",
        "  # Function that returns L2 regularisation loss for the given network\n",
        "  L = len(network)\n",
        "  ans = 0\n",
        "  for k in range(L):\n",
        "    ans += np.sum(network[k]['weights'] ** 2)\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def L1_regularisation(network):\n",
        "  # Function that returns L1 regularisation loss for the given network\n",
        "  L = len(network)\n",
        "  ans = 0\n",
        "  for k in range(L):\n",
        "    ans += np.sum(np.absolute(network[k]['weights']))\n",
        "  \n",
        "  return ans\n",
        "\n",
        "def grad_L2_regularisation(W):\n",
        "  # Function that returns L2 regularisation gradient for the given Weight matrix / tensor\n",
        "  return 2 * W\n",
        "\n",
        "def grad_L1_regularisation(W):\n",
        "  # Function that returns L1 regularisation gradient for the given Weight matrix / tensor\n",
        "  return np.sign(W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(Y_pred, Y_true):\n",
        "  # Function that returns cross entropy loss\n",
        "  assert(Y_pred.shape[1] == Y_true.shape[1])\n",
        "  M = Y_pred.shape[1]\n",
        "  return -(Y_true * np.log(Y_pred)).sum() / M\n",
        "\n",
        "def SquaredError(Y_pred, Y_true):\n",
        "  # Function that returns mean squared loss\n",
        "  assert(Y_pred.shape[1] == Y_true.shape[1])\n",
        "  M = Y_pred.shape[1]\n",
        "  return ((Y_true - Y_pred) ** 2).sum() / (2.0 * M)\n",
        "\n",
        "def overall_loss(network, Y_pred, Y_true, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation):\n",
        "  # Function that returns overall loss (Output layer cost function + Regularisation) for the network\n",
        "  return loss_fn(Y_pred, Y_true) + weight_decay * regularisation_fn(network)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6abLgZ7KRtM"
      },
      "source": [
        "def calc_accuracy_loss(network, X, Y, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation):\n",
        "  # Function that returns the accuracy and loss for a given network\n",
        "  Y_pred = forward_propagation(network, X)\n",
        "  loss = overall_loss(network, Y_pred, Y, loss_fn, weight_decay, regularisation_fn)\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  M = X.shape[1]\n",
        "  accuracy = np.sum(np.argmax(Y_pred, axis = 0) == np.argmax(Y, axis = 0)) / M\n",
        "  return accuracy, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def sgd_gradient_descent(X, Y, network, batch_size, eta, max_epochs, loss_fn = CrossEntropy_loss, weight_decay = 0, \n",
        "                         regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input training data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True training output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (training accuracy, average training loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (validation accuracy, average validation loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "  \n",
        "  for epoch in tqdm(range(max_epochs)):        \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "    \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size, eta, max_epochs, loss_fn = CrossEntropy_loss, beta = 0.9, weight_decay = 0, \n",
        "                              regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history (standard value 0.9 used)\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (training accuracy, average training loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (validation accuracy, average validation loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k]\n",
        "  \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n",
        "\n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size, eta, max_epochs, loss_fn = CrossEntropy_loss, beta = 0.9, weight_decay = 0, \n",
        "                              regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history (standard value 0.9 used)\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (training accuracy, average training loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (validation accuracy, average validation loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  lookahead_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred_org = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred_org, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      Y_pred = forward_propagation(lookahead_network, X[:,i:i+batch_size])\n",
        "      dw, db = back_propagation(lookahead_network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k] \n",
        "        lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "        lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    \n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "\n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def rmsprop_gradient_descent(X, Y, network, batch_size, eta, max_epochs, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9,\n",
        "                             weight_decay = 0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                             validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as decaying weight for learning rate update (standard value 0.9 used)\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (training accuracy, average training loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (validation accuracy, average validation loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "    \n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                     [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        batch += 1\n",
        "        Y_true = Y[:,i:i+batch_size]\n",
        "        Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "        batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "        dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "        batch_loss_values.append(batch_curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "        for k in range(L):\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "    \n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "\n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size, eta, max_epochs, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                          beta2 = 0.999, weight_decay = 0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                          validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update (standard value 0.9 used)\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update (standard value 0.999 used)\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (training accuracy, average training loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (validation accuracy, average validation loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  \n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, v_w, v_b, m_w, m_b, = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                                [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                                [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "  \n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "        batch += 1\n",
        "        Y_true = Y[:,i:i+batch_size]\n",
        "        Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "        batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "        dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "        batch_loss_values.append(batch_curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "        for k in range(L):\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lk1FkKdZj1"
      },
      "source": [
        "# Nadam optimisation followed from this article : https://arxiv.org/pdf/1609.04747.pdf\n",
        "def nadam_gradient_descent(X, Y, network, batch_size, eta, max_epochs, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, \n",
        "                           beta2 = 0.999, weight_decay = 0, regularisation_fn = L2_regularisation, grad_reglr_fn = grad_L2_regularisation, \n",
        "                           validX = None, validY = None):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nadam Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith column is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith column is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update (standard value 0.9 used)\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update (standard value 0.999 used)\n",
        "      weight_decay -- Hyperparameter scaling regularisation term\n",
        "      regularisation_fn -- (wat the name suggests)\n",
        "      grad_reglr_fn -- Function calculating gradient of the regularisation function\n",
        "      validX -- Validation input data (in same format as training data)\n",
        "      validY -- Validation output data (in same format as training data)\n",
        "      \n",
        "    Returns :\n",
        "      batch_loss_values -- List of average loss values after every batch of training\n",
        "      train_stats -- List of tuple (training accuracy, average training loss) of entire dataset after every epoch of training\n",
        "      valid_stats -- List of tuple (validation accuracy, average validation loss) of entire dataset after every epoch of training\n",
        "  \"\"\"\n",
        "  assert(X.shape[1] == Y.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "\n",
        "  batch_loss_values, train_stats, valid_stats = [], [], []\n",
        "  L = len(network)\n",
        "  batch_curr_loss, batch = 0, 0\n",
        "  dw, db, m_w, m_b, v_w, v_b = [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                               [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)], \\\n",
        "                               [np.zeros_like(network[k]['weights']) for k in range(L)], [np.zeros_like(network[k]['biases']) for k in range(L)]\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    for i in range(0, X.shape[1], batch_size):\n",
        "      batch += 1\n",
        "      Y_true = Y[:,i:i+batch_size]\n",
        "      Y_pred = forward_propagation(network, X[:,i:i+batch_size])\n",
        "      batch_curr_loss = overall_loss(network, Y_pred, Y_true, loss_fn, weight_decay, regularisation_fn)\n",
        "      dw, db = back_propagation(network, Y_true, Y_pred, weight_decay, grad_reglr_fn)\n",
        "      batch_loss_values.append(batch_curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'batch_loss': batch_curr_loss})\n",
        "      for k in range(L):\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "\n",
        "    train_stats.append(calc_accuracy_loss(network, X, Y, loss_fn, weight_decay, regularisation_fn))\n",
        "    if validX is not None:\n",
        "      valid_stats.append(calc_accuracy_loss(network, validX, validY, loss_fn, weight_decay, regularisation_fn))\n",
        "    \n",
        "    wandb.log({'epoch': epoch, 'validation_accuracy': valid_stats[-1][0] if validX is not None else 0, 'accuracy': train_stats[-1][0], \n",
        "              'validation_loss': valid_stats[-1][1] if validX is not None else 0, 'loss': train_stats[-1][1]})\n",
        "  \n",
        "  return batch_loss_values, train_stats, valid_stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOmXZeotoxbV"
      },
      "source": [
        "def convert_to_onehot(Y, N) :\n",
        "  # Converting output data Y to onehot representation\n",
        "  Y_onehot = []\n",
        "  for y in Y:\n",
        "    curr_y = [0] * N\n",
        "    curr_y[y] = 1\n",
        "    Y_onehot.append(curr_y)\n",
        "\n",
        "  return np.array(Y_onehot).T\n",
        "\n",
        "def train_validation_split(X, Y, train_to_valid_ratio = 0.9):\n",
        "  # Function to split the given input and output data into training and validation sets in the ratio given by train_to_valid_ratio\n",
        "  assert(train_to_valid_ratio > 0 and train_to_valid_ratio < 1)\n",
        "  np.random.seed(0)\n",
        "  perm = np.random.permutation(trainX.shape[0])\n",
        "  train_size = int(train_to_valid_ratio * len(perm))\n",
        "  train_indices = perm[:train_size]\n",
        "  valid_indices = perm[train_size:]\n",
        "\n",
        "  return X[train_indices], Y[train_indices], X[valid_indices], Y[valid_indices]\n",
        "\n",
        "def transform_NN_IO(X, Y, no_output_class):\n",
        "  # Function to transform the input and output to a form compatible with the Neural Network functions\n",
        "  X_norm = X.reshape(X.shape[0], (X.shape[1]*X.shape[2])).T / 255   # Input Training data with ith column being ith training example's data\n",
        "  Y_onehot = convert_to_onehot(Y, no_output_class)                  # Converting y labels to onehot representation\n",
        "\n",
        "  return X_norm, Y_onehot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv"
      },
      "source": [
        "# Encoding the functions with strings for using in wrapper functions to call them in an easy manner\n",
        "get_grad = {\n",
        "    'sigmoid' : grad_sigmoid,\n",
        "    'tanh' : grad_tanh,\n",
        "    'relu' : grad_relu\n",
        "}\n",
        "\n",
        "get_gd_function = {\n",
        "    'sgd' : sgd_gradient_descent, \n",
        "    'momentum' : momentum_gradient_descent, \n",
        "    'nesterov' : nesterov_gradient_descent, \n",
        "    'rmsprop' : rmsprop_gradient_descent, \n",
        "    'adam' : adam_gradient_descent, \n",
        "    'nadam' : nadam_gradient_descent \n",
        "}\n",
        "\n",
        "get_activ_fn = {\n",
        "    'sigmoid' : sigmoid,\n",
        "    'tanh' : tanh,\n",
        "    'relu' : relu\n",
        "}\n",
        "\n",
        "get_weight_init_fn = {\n",
        "    'random': random_initialisation, \n",
        "    'xavier': xavier_initialisation\n",
        "}\n",
        "\n",
        "get_regularisation_fn = {\n",
        "    'L2': L2_regularisation,\n",
        "    'L1': L1_regularisation\n",
        "}\n",
        "\n",
        "get_grad_reglr_fn = {\n",
        "    'L2': grad_L2_regularisation,\n",
        "    'L1': grad_L1_regularisation\n",
        "}\n",
        "\n",
        "get_output_grad_fn = {\n",
        "    'cross-entropy': Softmax_CrossEntropy_grad,\n",
        "    'squared-error': Softmax_SquaredError_grad\n",
        "}\n",
        "\n",
        "get_loss_fn = {\n",
        "    'cross-entropy': CrossEntropy_loss,\n",
        "    'squared-error': SquaredError\n",
        "}\n",
        "\n",
        "\n",
        "def train_NN(trainX, trainY, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn,\n",
        "             activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, \n",
        "             grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss, weight_decay = 0, regularisation_fn = L2_regularisation,\n",
        "             grad_reglr_fn = grad_L2_regularisation, validX = None, validY = None, testX = None, testY = None):\n",
        "  '''\n",
        "  Function that creates and trains the neural network for the given hyperparameters\n",
        "  Returns :\n",
        "    network -- List of dictionaries representing the neural network\n",
        "    train_stats -- List of tuple of the form (training accuracy, training loss) after each epoch of the training\n",
        "    valid_stats -- List of tuple of the form (validation accuracy, validation loss) after each epoch of the training \n",
        "                   (empty list returned if no validation set given)\n",
        "    test_stat -- Tuple (test accuracy, test loss) for the test set (None returned if no test set given)\n",
        "  ''' \n",
        "  \n",
        "  assert(trainX.shape[1] == trainY.shape[1])\n",
        "  if validY is not None:\n",
        "    assert(validX.shape[1] == validY.shape[1])\n",
        "  if testY is not None:\n",
        "    assert(testX.shape[1] == testY.shape[1])\n",
        "    \n",
        "  L = no_hidden_layers+1                                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [size_hidden_layer] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = trainX.shape[0]\n",
        "  n_L[L] = trainY.shape[0]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  batch_loss_values, train_stats, valid_stats = optimisation_fn(trainX, trainY, network, batch_size, learning_rate, max_epochs, \n",
        "                                                                loss_fn, weight_decay = weight_decay, regularisation_fn = regularisation_fn, \n",
        "                                                                grad_reglr_fn = grad_reglr_fn, validX = validX, validY = validY)\n",
        "  test_stat = None\n",
        "  if testY is not None: \n",
        "    test_stat = calc_accuracy_loss(network, testX, testY, loss_fn, weight_decay, regularisation_fn)\n",
        "    wandb.log({'test_accuracy': test_stat[0], 'test_loss': test_stat[1]})\n",
        "    # Printing the test accuracy and loss\n",
        "    print(f'Test set accuracy : {test_stat[0]} | Test set loss : {test_stat[1]}')\n",
        "\n",
        "  # Plotting the loss values after each batch of training\n",
        "  plt.plot(batch_loss_values)\n",
        "  plt.show()\n",
        "  # plt.plot(train_stats[:][0])\n",
        "  # plt.show()\n",
        "  # plt.plot(train_stats[:][1])\n",
        "  # plt.show()\n",
        "\n",
        "  return network, train_stats, valid_stats, test_stat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCamsnTLu6f0"
      },
      "source": [
        "def train_wrapper(trainX, trainY, optimizer, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, \n",
        "                  weight_initialisation, activation, loss, weight_decay = 0,\n",
        "                  validX = None, validY = None, testX = None, testY = None, regularisation = 'L2'):\n",
        "  '''\n",
        "  This is a wrapper for the train_NN function so as to easily train the neural networks with minimal arguments that are necessary.\n",
        "  Arguments :\n",
        "    trainX -- (matrix) Input training data matrix with data as column vectors\n",
        "    trainY -- (matrix) True training output data matrix with data as column vectors\n",
        "    optimizer -- (string) Optimisation function. Takes values only in ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "    batch_size -- (int) Batch size for training\n",
        "    learning_rate -- (float) Hyperparameter eta for gradient terms in training\n",
        "    max_epochs -- (int) Maximum number of epochs to train the neural network\n",
        "    no_hidden_layers -- (int) Number of hidden layers in the neural network\n",
        "    size_hidden_layer -- (int) Number of neurons in each hidden layer\n",
        "    weight_initialisation -- (string) Weight initialisation method. Takes values only in ['random', 'xavier']\n",
        "    activation -- (string) Activation function for each neuron. Takes values only in ['relu', 'sigmoid', 'tanh]\n",
        "    loss -- (string) Loss function used. Takes values only in ['cross-entropy', 'squared-error']\n",
        "    weight_decay -- (float) Hyperparameter lambda for regularisation term in training\n",
        "    validX -- (matrix) Input validation data matrix with data as column vectors\n",
        "    validY -- (matrix) True validation output data matrix with data as column vectors\n",
        "    testX -- (matrix) Input testing data matrix with data as column vectors\n",
        "    testY -- (matrix) True testing output data matrix with data as column vectors\n",
        "    regularisation -- (string) Type of regularisation used. Takes values only in ['L2', 'L1']\n",
        "  \n",
        "  Returns :\n",
        "    Output from train_NN function (train_stats, valid_stats, test_stat)\n",
        "  '''\n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({'no_hidden_layers': no_hidden_layers, \n",
        "                       'size_hidden_layer': size_hidden_layer,\n",
        "                       'batch_size': batch_size,\n",
        "                       'learning_rate': learning_rate,\n",
        "                       'no_epochs': max_epochs,\n",
        "                       'optimizer': optimizer,\n",
        "                       'weight_decay': weight_decay,\n",
        "                       'weight_initialisation': weight_initialisation, \n",
        "                       'activation_fn': activation,\n",
        "                       'regularisation': regularisation,\n",
        "                       'loss': loss\n",
        "                      })\n",
        "  \n",
        "  # Updating name of run\n",
        "  wandb.run.name = f'hl_{no_hidden_layers}_bs_{batch_size}_ac_{activation}_op_{optimizer}'\n",
        "  wandb.run.save()\n",
        "  \n",
        "  return train_NN(trainX, trainY, get_gd_function[optimizer], batch_size, learning_rate, \n",
        "                  max_epochs, no_hidden_layers, size_hidden_layer, get_weight_init_fn[weight_initialisation],\n",
        "                  get_activ_fn[activation], linear, softmax, get_grad[activation], get_output_grad_fn[loss], \n",
        "                  get_loss_fn[loss], weight_decay, get_regularisation_fn[regularisation], get_grad_reglr_fn[regularisation], \n",
        "                  validX, validY, testX, testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKQ4md7QrEuy"
      },
      "source": [
        "# Transforming dataset to a form that can be used in the neural network functions\n",
        "trainX_split, trainY_split, validX_split, validY_split = train_validation_split(trainX, trainy, 0.9)\n",
        "trainX_tr, trainY_tr = transform_NN_IO(trainX_split, trainY_split, N_CLASSES)\n",
        "validX_tr, validY_tr = transform_NN_IO(validX_split, validY_split, N_CLASSES)\n",
        "testX_tr, testY_tr = transform_NN_IO(testX, testy, N_CLASSES)\n",
        "\n",
        "# Questions 2,3 -- Training a neural network with given hyperparameters with cross entropy loss\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "# network, *_ = train_wrapper(trainX_tr, trainY_tr, 'adam', 64, 1e-3, 20, 3, 64, 'xavier', 'relu', 'cross-entropy', \n",
        "#                             0.0005, validX_tr, validY_tr, testX_tr, testY_tr, 'L2')\n",
        "# run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',                   # Possible search : grid, random, bayes\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'no_epochs': {\n",
        "            'values': [5, 10]\n",
        "        },\n",
        "        'no_hidden_layers': {\n",
        "            'values': [3, 4, 5]\n",
        "        },\n",
        "        'size_hidden_layer': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'weight_decay' :{\n",
        "            'values': [0, 0.0005, 0.005]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-1, 1e-2, 1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam' ]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [16, 32, 64]\n",
        "        },\n",
        "        'weight_initialisation': {\n",
        "            'values': ['random', 'xavier']\n",
        "        },\n",
        "        'activation_fn': {\n",
        "            'values': ['relu', 'tanh', 'sigmoid']\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYFa9Jh7rU0"
      },
      "source": [
        "def sweep_wrapper(trainX, trainY, validX = None, validY = None, testX = None, testY = None, loss = 'cross-entropy'):\n",
        "  # Wrapper function to call the train_NN function for sweeping with different hyperparameters\n",
        "  # loss - (string) Loss function used. Takes values only in ['cross-entropy', 'squared-error']\n",
        "\n",
        "  # Default values for hyper-parameters we're going to sweep over\n",
        "  config_defaults =  {\n",
        "      'no_epochs': 10,\n",
        "      'no_hidden_layers': 4,\n",
        "      'size_hidden_layer': 128, \n",
        "      'weight_decay' : 0,\n",
        "      'learning_rate': 1e-3,\n",
        "      'optimizer': 'adam',\n",
        "      'batch_size': 64,\n",
        "      'weight_initialisation': 'random', \n",
        "      'activation_fn': 'relu',\n",
        "      'regularisation': 'L2',\n",
        "      'loss': loss\n",
        "  }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  run = wandb.init(config=config_defaults, reinit=True)\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "  wandb.config.update({'no_classes': N_CLASSES})\n",
        "\n",
        "  wandb.run.name = f'hl_{config.no_hidden_layers}_bs_{config.batch_size}_ac_{config.activation_fn}'\n",
        "  wandb.run.save()\n",
        "\n",
        "  # Sweep uses L2 regularisation as default as given in the question\n",
        "  train_NN(trainX, trainY, get_gd_function[config.optimizer], config.batch_size, config.learning_rate, \n",
        "           config.no_epochs, config.no_hidden_layers, config.size_hidden_layer, get_weight_init_fn[config.weight_initialisation],\n",
        "           get_activ_fn[config.activation_fn], linear, softmax, get_grad[config.activation_fn], \n",
        "           get_output_grad_fn[loss], get_loss_fn[loss], config.weight_decay, L2_regularisation, grad_L2_regularisation, \n",
        "           validX, validY, testX, testY)\n",
        "  \n",
        "  run.finish()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTMh4OI7HGzC"
      },
      "source": [
        "# Questions 4-6 -- Running a sweep across different hyperparameter configurations with cross entropy as the loss function\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity=\"abisheks\", project=\"assignment1\")\n",
        "# wandb.agent(sweep_id, lambda : sweep_wrapper(trainX_tr, trainY_tr, validX_tr, validY_tr, testX_tr, testY_tr, 'cross-entropy'))\n",
        "\n",
        "# Question 6 (second part)\n",
        "# The question asks for a recommendation for what configuration to use to get close to 95% accuracy.\n",
        "# We tried the following setting and other modifications including increasing epochs to 60 and hidden layer size to 512.\n",
        "# We reached a train accuracy of close to 94 %, but the validation and test accuracy stay close to 88%. \n",
        "# This implies that the model may overfit if we increase the complexity further\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "# network, *_ = train_wrapper(trainX_tr, trainY_tr, 'adam', 256, 5e-4, 40, 4, 256, 'xavier', 'relu', 'cross-entropy', \n",
        "#                            0.0005, validX_tr, validY_tr, testX_tr, testY_tr, 'L2')\n",
        "\n",
        "\n",
        "# Future steps to increase test accuracy are to increase the quantity and quality of training data by using methods like Data augmentation.\n",
        "# (this will increase the size of the training data which might be useful for training the higher number of neurons\n",
        "#  in a way that will generalise well to new examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHHZemXkdwVb"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(network, X, Y):\n",
        "  # Function to plot the confusion matrix for a given trained network, input (X) and true output (Y)\n",
        "  Y_pred = forward_propagation(network, X)\n",
        "  cm = confusion_matrix(np.argmax(Y, axis=0), np.argmax(Y_pred, axis=0))\n",
        "\n",
        "  df_cm = pd.DataFrame(cm, [IMG_LABELS[i] for i in range(cm.shape[0])], [IMG_LABELS[i] for i in range(cm.shape[0])])\n",
        "  \n",
        "  # Plotting the confusion matrix\n",
        "  plt.figure(figsize=(10,10))\n",
        "  sn.set(font_scale=1.0) # for label size\n",
        "  sn.heatmap(df_cm, annot=True, \n",
        "            annot_kws={\"size\": 16}, cmap='YlGnBu', fmt='g'\n",
        "            )\n",
        "  plt.ylabel(r'$Y_{true}$', fontsize=18)\n",
        "  plt.xlabel(r'$Y_{pred}$', fontsize=18)\n",
        "\n",
        "  # Logging the confusion matrix to WANDB\n",
        "  wandb.log({'Confusion Matrix': wandb.Image(plt, caption = 'hl_4_bs_64_ac_relu_op_momentum')})\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "# Question 7 -- Training the neural network and printing the test accuracy and loss for the best choice of hyperparameters with cross entropy loss.\n",
        "#               And plotting the confusion matrix for the output of this trained network\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "network, *_ = train_wrapper(trainX_tr, trainY_tr, 'momentum', 64, 1e-2, 10, 4, 128, 'xavier', 'relu', 'cross-entropy', \n",
        "                            0.0005, validX_tr, validY_tr, testX_tr, testY_tr, 'L2')\n",
        "plot_confusion_matrix(network, testX_tr, testY_tr)\n",
        "run.finish()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoJeGwK0dix_"
      },
      "source": [
        "# Questions 8 -- Running a sweep across different hyperparameter configurations with squared error as the loss function\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep_config, entity=\"abisheks\", project=\"assignment1\")\n",
        "# wandb.agent(sweep_id, lambda : sweep_wrapper(trainX_tr, trainY_tr, validX_tr, validY_tr, testX_tr, testY_tr, 'squared-error'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waci5hIhd7hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1404b712-26c7-4029-b92d-f2258920fdeb"
      },
      "source": [
        "# Loading the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX_2, trainy_2), (testX_2, testy_2) = mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "\n",
        "# Number of classes in the MNIST (Digit) dataset\n",
        "N_CLASSES_2 = np.unique(trainy_2).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Transforming dataset to a form that can be used in the neural network functions\n",
        "trainX_split_2, trainY_split_2, validX_split_2, validY_split_2 = train_validation_split(trainX_2, trainy_2, 0.9)\n",
        "trainX_tr_2, trainY_tr_2 = transform_NN_IO(trainX_split_2, trainY_split_2, N_CLASSES_2)\n",
        "validX_tr_2, validY_tr_2 = transform_NN_IO(validX_split_2, validY_split_2, N_CLASSES_2)\n",
        "testX_tr_2, testY_tr_2 = transform_NN_IO(testX_2, testy_2, N_CLASSES_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdlsRGUiWibN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2314a71b1f1441d790b0c8656f717fe2",
            "86e08516f6cf45bb92db05bf75d16ea4",
            "08f0ea3dddd24aa58d053e1a036a46fc",
            "91d1286314db4acc90b93b902e839829",
            "a4bf0d072de644fa90a2d982c45c0620",
            "3c44ef1432fc473ab1a21cc4412bab37",
            "0ba7f56b59184163a5033e7982b222bd",
            "8c50745a0cbe4a15b9a3c0113e329fac",
            "ffc15169cc6245bb849505b3b329aa0d",
            "b6eaa1156304425e916caf15c2594bcf",
            "2b5cd152ba5145bd8e5144a0216cb284",
            "b58a4730c25547e0a8ca0a4d9e084549",
            "e8a594f4e67044818f508527a7ff0d1a",
            "07f18ef686cc4e959e2fedd01a39f06f",
            "69c2c7ec66ba4bb8ab5818ef323e676e",
            "06e8ad8e13334a9497afff699e47c8b6",
            "5aa4039fadd140f9953b3d04e2d11b6c",
            "17f7e28481904bc2bbc2b304f435e91c",
            "4421ceae205f4263a4f6ede77e0e96c3",
            "25e2f89caffd437898744f74731dc446",
            "7331485abfa34577b249534fa3bf47fb",
            "709e8094010449de8c706be9208bfc42",
            "c05a7836d1324718a0440075696ab76a",
            "da2d927496674483a66b79b1a73e34ff"
          ]
        },
        "outputId": "11887734-0a9a-4019-b8f5-4ba717965744"
      },
      "source": [
        "# Training neural networks for the MNIST dataset\n",
        "\n",
        "def do_mnist_cross_entropy(trainX, trainY, validX = None, validY = None, testX = None, testY = None):\n",
        "  # Function to train neural network (with cross entropy loss function) for MNIST dataset with three best configuration of hyperparameters, \n",
        "  # predicted from experiments on fashion-MNIST \n",
        "  run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "  network, *_ = train_wrapper(trainX, trainY, 'momentum', 64, 1e-2, 10, 4, 128, 'xavier', 'relu', 'cross-entropy', \n",
        "                              0.0005, validX, validY, testX, testY, 'L2')\n",
        "  run.finish()\n",
        "\n",
        "  run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "  network, *_ = train_wrapper(trainX, trainY, 'momentum', 64, 1e-2, 10, 4, 64, 'xavier', 'relu', 'cross-entropy', \n",
        "                              0.0005, validX, validY, testX, testY, 'L2')\n",
        "  run.finish()\n",
        "\n",
        "  run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "  network, *_ = train_wrapper(trainX, trainY, 'nadam', 64, 1e-3, 10, 4, 64, 'xavier', 'relu', 'cross-entropy', \n",
        "                              0.0005, validX, validY, testX, testY, 'L2')\n",
        "  run.finish()\n",
        "\n",
        "\n",
        "def do_mnist_squared_error(trainX, trainY, validX = None, validY = None, testX = None, testY = None):\n",
        "  # Function to train neural network (with cross entropy loss function) for MNIST dataset with three best configuration of hyperparameters, \n",
        "  # predicted from experiments on fashion-MNIST \n",
        "  run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "  network, *_ = train_wrapper(trainX, trainY, 'adam', 64, 1e-3, 10, 4, 128, 'random', 'relu', 'squared-error', \n",
        "                              0, validX, validY, testX, testY, 'L2')\n",
        "  run.finish()\n",
        "\n",
        "  run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "  network, *_ = train_wrapper(trainX, trainY, 'nadam', 64, 1e-3, 10, 4, 128, 'random', 'relu', 'squared-error', \n",
        "                              0, validX, validY, testX, testY, 'L2')\n",
        "  run.finish()\n",
        "\n",
        "  run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "  network, *_ = train_wrapper(trainX, trainY, 'momentum', 64, 1e-1, 10, 4, 128, 'xavier', 'relu', 'squared-error', \n",
        "                              0, validX, validY, testX, testY, 'L2')\n",
        "  run.finish()\n",
        "\n",
        "\n",
        "# Question 10 -- Training neural networks on MNIST dataset after experimenting with fashion-MNIST dataset and figuring out 3 best hyperparameter configurations\n",
        "# ---------------- Uncomment the below code to run -----------------\n",
        "\n",
        "# # For cross entropy loss\n",
        "# do_mnist_cross_entropy(trainX_tr_2, trainY_tr_2, validX_tr_2, validY_tr_2, testX_tr_2, testY_tr_2)\n",
        "# # For squared error loss\n",
        "# do_mnist_squared_error(trainX_tr_2, trainY_tr_2, validX_tr_2, validY_tr_2, testX_tr_2, testY_tr_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabisheks\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">mild-sun-1123</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1pcsarn8\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1pcsarn8</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210312_213735-1pcsarn8</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "100%|██████████| 10/10 [01:18<00:00,  7.87s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy : 0.9663 | Test set loss : 0.24914074844187134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 121<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2314a71b1f1441d790b0c8656f717fe2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210312_213735-1pcsarn8/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210312_213735-1pcsarn8/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>8440</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>batch_loss</td><td>0.16546</td></tr><tr><td>_runtime</td><td>81</td></tr><tr><td>_timestamp</td><td>1615585136</td></tr><tr><td>_step</td><td>8450</td></tr><tr><td>validation_accuracy</td><td>0.9645</td></tr><tr><td>accuracy</td><td>0.97811</td></tr><tr><td>validation_loss</td><td>0.25806</td></tr><tr><td>loss</td><td>0.20848</td></tr><tr><td>test_accuracy</td><td>0.9663</td></tr><tr><td>test_loss</td><td>0.24914</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>batch_loss</td><td>█▇▇▄▅▄▄▃▅▃▃▃▂▄▃▄▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁▆▇▇▇▇████</td></tr><tr><td>accuracy</td><td>▁▅▆▇▇█████</td></tr><tr><td>validation_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr><tr><td>loss</td><td>█▆▄▄▃▂▂▂▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">mild-sun-1123</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1pcsarn8\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1pcsarn8</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">swift-sky-1124</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1suedywj\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1suedywj</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210312_213859-1suedywj</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "100%|██████████| 10/10 [00:44<00:00,  4.46s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy : 0.9717 | Test set loss : 0.20483703505742404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 153<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffc15169cc6245bb849505b3b329aa0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210312_213859-1suedywj/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210312_213859-1suedywj/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>8440</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>batch_loss</td><td>0.12735</td></tr><tr><td>_runtime</td><td>47</td></tr><tr><td>_timestamp</td><td>1615585186</td></tr><tr><td>_step</td><td>8450</td></tr><tr><td>validation_accuracy</td><td>0.9715</td></tr><tr><td>accuracy</td><td>0.98456</td></tr><tr><td>validation_loss</td><td>0.20729</td></tr><tr><td>loss</td><td>0.16455</td></tr><tr><td>test_accuracy</td><td>0.9717</td></tr><tr><td>test_loss</td><td>0.20484</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>batch_loss</td><td>█▆▇▄▅▅▄▃▆▄▄▃▂▄▂▃▂▃▂▂▂▁▂▂▂▃▂▃▂▂▂▂▂▁▁▂▂▁▁▂</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▇▇▇▇█▇█</td></tr><tr><td>accuracy</td><td>▁▃▄▆▆▇▇▇██</td></tr><tr><td>validation_loss</td><td>█▆▅▃▃▂▂▂▁▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">swift-sky-1124</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1suedywj\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1suedywj</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">peachy-snowflake-1125</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1l2m2aqw\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1l2m2aqw</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210312_213950-1l2m2aqw</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
            "100%|██████████| 10/10 [00:48<00:00,  4.85s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy : 0.9703 | Test set loss : 0.2011605252209507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 191<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aa4039fadd140f9953b3d04e2d11b6c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210312_213950-1l2m2aqw/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210312_213950-1l2m2aqw/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>8440</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>batch_loss</td><td>0.13801</td></tr><tr><td>_runtime</td><td>50</td></tr><tr><td>_timestamp</td><td>1615585240</td></tr><tr><td>_step</td><td>8450</td></tr><tr><td>validation_accuracy</td><td>0.969</td></tr><tr><td>accuracy</td><td>0.98169</td></tr><tr><td>validation_loss</td><td>0.20531</td></tr><tr><td>loss</td><td>0.1628</td></tr><tr><td>test_accuracy</td><td>0.9703</td></tr><tr><td>test_loss</td><td>0.20116</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>batch_loss</td><td>█▆█▃▅▄▄▃▆▅▄▂▂▅▂▄▁▃▃▃▂▁▂▂▂▄▃▃▃▃▃▃▃▁▂▂▁▁▁▃</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_accuracy</td><td>▁▅▆▇▇▇▇██▇</td></tr><tr><td>accuracy</td><td>▁▅▆▇▇▇▇██▇</td></tr><tr><td>validation_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">peachy-snowflake-1125</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1l2m2aqw\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1l2m2aqw</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}