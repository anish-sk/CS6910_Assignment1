{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DMWBsOgIyM-",
        "outputId": "3d8b3460-8412-42e9-bdbe-bb82eb237a04"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 39.1MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=a3d08f8d63dd7aa6955dbe40ac530cdd5c29618ec0b14744f87a30eb15140b04\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=9e81b8456b35c27f1364502392aa1b84c00724ed6f190c78013da204f967e677\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: sentry-sdk, configparser, shortuuid, subprocess32, docker-pycreds, smmap, gitdb, GitPython, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "Pd2ZVTXmJfT6",
        "outputId": "5e38567d-d1ea-495c-c62b-cc638b59029c"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"assignment1\", entity=\"abisheks\")\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabisheks\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dry-donkey-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/2kaxpxcy\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/2kaxpxcy</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210304_090706-2kaxpxcy</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def sigmoid(x):\n",
        "  #Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def softmax(x):\n",
        "  #Calculates the softmax function\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def linear(W, X, b):\n",
        "  #Calculates the linear function\n",
        "  return W @ X + b"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f2c66a-a1a3-4eb0-ee25-325c4c2d62ec"
      },
      "source": [
        "def initialize_network(n_L, preActFns_L, actFns_L):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "  n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "  preActFns_L -- an array who ith element is the Pre Activation function of (i+1)th layer of the neural network\n",
        "  actFns_L -- an array who ith element is the Activation function of (i+1)th layer of the neural network\n",
        "\n",
        "  Returns :\n",
        "  network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':np.random.randn(n_L[i], n_L[i-1])*0.01,    # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of ith layer\n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propogation (will be filled in forward-propogation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "  return network\n",
        "\n",
        "\n",
        "wandb.config.update({\"n_hidden_layers\": 1, \"size_hidden_layer\":4})   # Setting the hyperparameters in the wandb\n",
        "L = wandb.config['n_hidden_layers']+1    # Number of hidden layerws + Output layer in the neural network\n",
        "n_L = [wandb.config['size_hidden_layer']] * (L+1)    # List of number of neurons in the neural network\n",
        "\n",
        "n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "n_L[L] = N_CLASSES\n",
        "\n",
        "pre_act_fns_L = [linear] * L    # List of Pre-activation functions of the hidden layers and output layer\n",
        "act_fns_L = [sigmoid] * (L-1) + [softmax]   # List of Activation functions of the hidden layers and output layer\n",
        "\n",
        "network = initialize_network(n_L, pre_act_fns_L, act_fns_L)\n",
        "print(network)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'weights': array([[ 0.0032712 , -0.00161081,  0.0048104 , ..., -0.00826681,\n",
            "         0.00903995, -0.00647184],\n",
            "       [ 0.00390153, -0.00705483, -0.00440369, ..., -0.00819731,\n",
            "        -0.00948209,  0.00021856],\n",
            "       [-0.00088553,  0.00687713,  0.00025081, ...,  0.01351832,\n",
            "         0.00033313, -0.00273922],\n",
            "       [ 0.00202317, -0.0158685 ,  0.0043207 , ...,  0.01137318,\n",
            "        -0.00314253,  0.00430464]]), 'biases': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'pre_activation_fn': <function linear at 0x7fcab0d5a050>, 'activation_fn': <function sigmoid at 0x7fcab0cd6560>, 'no_neurons': 4, 'cache': []}, {'weights': array([[-0.0095319 , -0.01199536,  0.00450282, -0.00555743],\n",
            "       [-0.01330772, -0.00169307, -0.00654462, -0.00255923],\n",
            "       [ 0.00540258, -0.00703787, -0.00949956, -0.0021139 ],\n",
            "       [-0.01660067,  0.00929078,  0.00799257, -0.00748297],\n",
            "       [-0.00802371,  0.00667073, -0.00229169, -0.00059968],\n",
            "       [ 0.01247042, -0.00737659,  0.00351498,  0.01337076],\n",
            "       [-0.01722931,  0.01618925, -0.00089087,  0.00536924],\n",
            "       [ 0.00979426,  0.01916303, -0.00219872,  0.00200715],\n",
            "       [ 0.00783997,  0.00610744,  0.00422963,  0.01540785],\n",
            "       [-0.01010686, -0.00314189, -0.00624233, -0.01025281]]), 'biases': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'pre_activation_fn': <function linear at 0x7fcab0d5a050>, 'activation_fn': <function softmax at 0x7fcab0cd6950>, 'no_neurons': 10, 'cache': []}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "  print(W.shape, H_prev.shape, b.shape)\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "\n",
        "  assert(A.shape == (W.shape[0], H_prev.shape[1]))\n",
        "  pre_act_cache = A   # Caching the pre-activation ouptut to be used in backpropogation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "\n",
        "  assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
        "  cache = (pre_activation_cache, H)   # Caching the pre-activation and activation output to use it in back-propogation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVFXPTolPf5v",
        "outputId": "de5c8e71-7be2-4248-9b04-9eab96da91f0"
      },
      "source": [
        "def forward_propogation(X, network):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "    X -- Input data as a matrix where ith column is the ith input data from the training example\n",
        "    network -- Given neural network (as an array of dictionaries)\n",
        "    \n",
        "    Returns :\n",
        "    H -- Output from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    H = X                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "    \n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache\n",
        "    \n",
        "    assert(H.shape == (network[L-1]['no_neurons'], X.shape[1]))\n",
        "            \n",
        "    return H\n",
        "\n",
        "trainX_reshaped = trainX.reshape((trainX.shape[1]*trainX.shape[2], trainX.shape[0]))\n",
        "HL = forward_propogation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "print(HL)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 784) (784, 60000) (4, 1)\n",
            "(10, 4) (4, 60000) (10, 1)\n",
            "[[1.63623888e-06 1.64214243e-06 1.65513263e-06 ... 1.66853923e-06\n",
            "  1.64997392e-06 1.64199663e-06]\n",
            " [1.64557863e-06 1.63653689e-06 1.64898198e-06 ... 1.66792420e-06\n",
            "  1.66762769e-06 1.66397609e-06]\n",
            " [1.66756311e-06 1.65414991e-06 1.68007935e-06 ... 1.67213409e-06\n",
            "  1.65896730e-06 1.65587192e-06]\n",
            " ...\n",
            " [1.71943325e-06 1.71678149e-06 1.68770052e-06 ... 1.67342856e-06\n",
            "  1.70357590e-06 1.70685706e-06]\n",
            " [1.69490682e-06 1.70102191e-06 1.68429280e-06 ... 1.67400855e-06\n",
            "  1.68519566e-06 1.70744315e-06]\n",
            " [1.64849654e-06 1.63985281e-06 1.65425632e-06 ... 1.66805580e-06\n",
            "  1.66327672e-06 1.64883753e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}