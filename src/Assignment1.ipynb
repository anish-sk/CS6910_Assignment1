{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"assignment1\", entity=\"abisheks\")\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def sigmoid(x):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def softmax(x):\n",
        "  # Calculates the softmax function\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def linear(W, x, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ x + b\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def grad_tanh(x):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(y_pred, y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(y - y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising randomly structure with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "trainX_reshaped = trainX.reshape((trainX.shape[1]*trainX.shape[2], trainX.shape[0])) # Input Training data with ith column being ith training example's data\n",
        "\n",
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "# wandb.config.update({\"n_hidden_layers\": 3, \"size_hidden_layer\":32})    # Setting the hyperparameters in the wandb\n",
        "# L = wandb.config['n_hidden_layers']+1                                 # Number of hidden layerws + Output layer in the neural network\n",
        "# n_L = [wandb.config['size_hidden_layer']] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "# n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "# n_L[L] = N_CLASSES\n",
        "\n",
        "# pre_act_fns_L = [linear] * L                  # List of Pre-activation functions of the hidden layers and output layer\n",
        "# act_fns_L = [sigmoid] * (L-1) + [softmax]     # List of Activation functions of the hidden layers and output layer\n",
        "# grad_act_fns_L = [grad_sigmoid] * (L-1)       # List of Gradients of the Activation functions, of the hidden layers\n",
        "# grad_output_fn = Softmax_CrossEntropy_grad\n",
        "\n",
        "# network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, random_initialisation)\n",
        "# print(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = A   # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  H_prev = H_prev.reshape((H_prev.shape[0], 1))\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  H = H.reshape((H.shape[0],1))\n",
        "  cache = (pre_activation_cache, H)   # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, x):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      x -- Input data from the training set\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    H = x                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "    cache_prev = (x,x) \n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache_prev\n",
        "        cache_prev = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "            \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, y, y_pred):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      y -- True output corresponding to the training data input\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](y_pred, y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = grad_a_k_L @ network[k]['cache'][1].T\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = grad_a_k_L\n",
        "\n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k]['cache'][0])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(y_pred, y_true):\n",
        "  return -(y_true * np.log(y_pred)).sum()\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "  return ((y_true - y_pred) ** 2).sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def vanilla_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  batch = 0\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    dw, db = [0] * L, [0] * L\n",
        "    for k in range(L):\n",
        "      dw[k] = np.zeros_like(network[k]['weights'])\n",
        "      db[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "    curr_loss, counter = 0, 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        print(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          network[k]['weights'] -= eta * dw[k]\n",
        "          network[k]['biases'] -= eta * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      print(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "  \n",
        "  return loss_values\n",
        "\n",
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  batch = 0\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "    dw, db = [0] * L, [0] * L\n",
        "    for k in range(L):\n",
        "      dw[k] = np.zeros_like(network[k]['weights'])\n",
        "      db[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "    curr_loss, counter = 0, 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        print(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          network[k]['weights'] -= eta * dw[k]\n",
        "          network[k]['biases'] -= eta * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      print(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "  \n",
        "  return loss_values\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv"
      },
      "source": [
        "def train_NN(X, Y, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn, activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss):\n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"max_epochs\": max_epochs\n",
        "                      })\n",
        "    \n",
        "  L = wandb.config['no_hidden_layers']+1                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [wandb.config['size_hidden_layer']] * (L+1)     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = X.shape[1]\n",
        "  n_L[L] = Y.shape[1]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  loss_values = optimisation_fn(X, Y, network, batch_size, learning_rate, max_epochs, loss_fn)\n",
        "\n",
        "  plt.plot(loss_values)\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "trainy_onehot = []\n",
        "for y in trainy:\n",
        "  curr_y = [0]*N_CLASSES\n",
        "  curr_y[y] = 1\n",
        "  trainy_onehot.append(curr_y)\n",
        "\n",
        "trainy_onehot = np.array(trainy_onehot)\n",
        "\n",
        "train_NN(trainX_reshaped.T, trainy_onehot, vanilla_gradient_descent, 64, 3e-2, 200, 3, 32, random_initialisation, sigmoid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}