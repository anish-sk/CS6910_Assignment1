{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e45afb053da4f19a959beb5cf4d9de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8be5620909254416942e20b7c5181082",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4155e6e2049b47d7b47b071d3537563b",
              "IPY_MODEL_bb455db85dc64229a9b5482da41ffa98"
            ]
          }
        },
        "8be5620909254416942e20b7c5181082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4155e6e2049b47d7b47b071d3537563b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_aa7aea0ec650486d82d7321db3b755e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.02MB of 0.02MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dfd99fb3c0ea465cb8c5b2eefed6dd99"
          }
        },
        "bb455db85dc64229a9b5482da41ffa98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ec3ca5bb287450d958beaa487a38999",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b66831889f8a4facac14a06a6d40aec9"
          }
        },
        "aa7aea0ec650486d82d7321db3b755e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dfd99fb3c0ea465cb8c5b2eefed6dd99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ec3ca5bb287450d958beaa487a38999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b66831889f8a4facac14a06a6d40aec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7998333b253a4d8aaf8a3129f5e69f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d095c578934346c68c8b18043de13ab5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14d62c1812b44d67b465a5365bac21c6",
              "IPY_MODEL_208d6037aa014340a5b68b9e61c6e5c3"
            ]
          }
        },
        "d095c578934346c68c8b18043de13ab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14d62c1812b44d67b465a5365bac21c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_bf8ae20e603749f3b65487ff6e137e8a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_181425eedeef4a86a85c9a4e0408eae4"
          }
        },
        "208d6037aa014340a5b68b9e61c6e5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b60494e71a1145449fac0f665ca42b93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_76d749bbfae0407fbc169b4c7cd2df1f"
          }
        },
        "bf8ae20e603749f3b65487ff6e137e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "181425eedeef4a86a85c9a4e0408eae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b60494e71a1145449fac0f665ca42b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "76d749bbfae0407fbc169b4c7cd2df1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd213fd-5242-4126-e872-05e7d3932f4e"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\r\u001b[K     |▏                               | 10kB 21.3MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 27.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 21.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 24.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 23.8MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 26.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 18.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 19.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 18.1MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 18.1MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 163kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 18.1MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 18.1MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 204kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 235kB 18.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 18.1MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 276kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 296kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 307kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 327kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 348kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 368kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 378kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 389kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 399kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 409kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 419kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 430kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 440kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 460kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 471kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 481kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 491kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 501kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 522kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 532kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 542kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 552kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 563kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 583kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 593kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 604kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 614kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 624kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 634kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 645kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 655kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 665kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 675kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 686kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 696kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 706kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 716kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 727kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 737kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 747kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 757kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 778kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 788kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 798kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 808kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 819kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 829kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 839kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 849kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 860kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 870kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 880kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 890kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 901kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 911kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 921kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 931kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 942kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 952kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 962kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 972kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 983kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 993kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.3MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.7MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.9MB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 18.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.7MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.7MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\u001b[0m\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=cd6977289a4662d352e5aa2d5e4b8c84b100c039ac69a973bb22f7b35748963c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=e0173adfb5eb93842f586a3139c70e7197fe0d0f8536cd4570f846cec9830773\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: shortuuid, subprocess32, docker-pycreds, pathtools, configparser, smmap, gitdb, GitPython, sentry-sdk, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602,
          "referenced_widgets": [
            "3e45afb053da4f19a959beb5cf4d9de1",
            "8be5620909254416942e20b7c5181082",
            "4155e6e2049b47d7b47b071d3537563b",
            "bb455db85dc64229a9b5482da41ffa98",
            "aa7aea0ec650486d82d7321db3b755e7",
            "dfd99fb3c0ea465cb8c5b2eefed6dd99",
            "3ec3ca5bb287450d958beaa487a38999",
            "b66831889f8a4facac14a06a6d40aec9"
          ]
        },
        "outputId": "4f4e4e47-0ff6-430e-f5a8-2f31cbb06ce4"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1u1k3k0w) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 168<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e45afb053da4f19a959beb5cf4d9de1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210307_074533-1u1k3k0w/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210307_074533-1u1k3k0w/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>4</td></tr><tr><td>_timestamp</td><td>1615103137</td></tr><tr><td>_step</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 30 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">crisp-forest-43</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1u1k3k0w\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1u1k3k0w</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1u1k3k0w). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sleek-vortex-46</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/uh0bzfyl\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/uh0bzfyl</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210307_080816-uh0bzfyl</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)\n",
        "run.finish()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def sigmoid(x):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def softmax(x):\n",
        "  # Calculates the softmax function\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def linear(W, x, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ x + b\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def grad_tanh(x):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(y_pred, y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(y - y_pred)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising randomly structure with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "trainX_reshaped = trainX.reshape((trainX.shape[1]*trainX.shape[2], trainX.shape[0])) # Input Training data with ith column being ith training example's data\n",
        "\n",
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "# wandb.config.update({\"n_hidden_layers\": 3, \"size_hidden_layer\":32})    # Setting the hyperparameters in the wandb\n",
        "# L = wandb.config['n_hidden_layers']+1                                 # Number of hidden layerws + Output layer in the neural network\n",
        "# n_L = [wandb.config['size_hidden_layer']] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "# n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "# n_L[L] = N_CLASSES\n",
        "\n",
        "# pre_act_fns_L = [linear] * L                  # List of Pre-activation functions of the hidden layers and output layer\n",
        "# act_fns_L = [sigmoid] * (L-1) + [softmax]     # List of Activation functions of the hidden layers and output layer\n",
        "# grad_act_fns_L = [grad_sigmoid] * (L-1)       # List of Gradients of the Activation functions, of the hidden layers\n",
        "# grad_output_fn = Softmax_CrossEntropy_grad\n",
        "\n",
        "# network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, random_initialisation)\n",
        "# print(network)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = A   # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  H_prev = H_prev.reshape((H_prev.shape[0], 1))\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  H = H.reshape((H.shape[0],1))\n",
        "  cache = (pre_activation_cache, H)   # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, x):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      x -- Input data from the training set\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    H = x                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "    cache_prev = (x,x) \n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache_prev\n",
        "        cache_prev = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "            \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, y, y_pred):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      y -- True output corresponding to the training data input\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](y_pred, y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = grad_a_k_L @ network[k]['cache'][1].T\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = grad_a_k_L\n",
        "\n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k]['cache'][0])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(y_pred, y_true):\n",
        "  return -(y_true * np.log(y_pred)).sum()\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "  return ((y_true - y_pred) ** 2).sum()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def vanilla_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db = [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          network[k]['weights'] -= eta * dw[k]\n",
        "          network[k]['biases'] -= eta * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "  \n",
        "  return loss_values\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  return loss_values\n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  tmp_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred_org = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred_org, y)\n",
        "      y_pred = forward_propagation(tmp_network, x)\n",
        "      grad_w_L, grad_b_L = back_propagation(tmp_network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          tmp_network[k]['weights'] += beta * m_w[k]\n",
        "          tmp_network[k]['biases'] += beta * m_b[k] \n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= beta * m_w[k]\n",
        "          network[k]['biases'] -= beta * m_b[k] \n",
        "          tmp_network[k]['weights'] -= (1+beta) * m_w[k]\n",
        "          tmp_network[k]['biases'] -= (1+beta) * m_b[k] \n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        tmp_network[k]['weights'] += beta * m_w[k]\n",
        "        tmp_network[k]['biases'] += beta * m_b[k] \n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= beta * m_w[k]\n",
        "        network[k]['biases'] -= beta * m_b[k] \n",
        "        tmp_network[k]['weights'] -= (1+beta) * m_w[k]\n",
        "        tmp_network[k]['biases'] -= (1+beta) * m_b[k] \n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  return loss_values\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def RMSProp_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  return loss_values"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
        "  \"\"\"\n",
        "    Trains the neural network using adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  return loss_values"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7998333b253a4d8aaf8a3129f5e69f09",
            "d095c578934346c68c8b18043de13ab5",
            "14d62c1812b44d67b465a5365bac21c6",
            "208d6037aa014340a5b68b9e61c6e5c3",
            "bf8ae20e603749f3b65487ff6e137e8a",
            "181425eedeef4a86a85c9a4e0408eae4",
            "b60494e71a1145449fac0f665ca42b93",
            "76d749bbfae0407fbc169b4c7cd2df1f"
          ]
        },
        "id": "sy7GHtEA3Kwv",
        "outputId": "12b48975-c399-4f85-9376-1516aef5b65a"
      },
      "source": [
        "def train_NN(X, Y, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn, activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss):\n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"max_epochs\": max_epochs\n",
        "                      })\n",
        "    \n",
        "  L = wandb.config['no_hidden_layers']+1                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [wandb.config['size_hidden_layer']] * (L+1)     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = X.shape[1]\n",
        "  n_L[L] = Y.shape[1]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  loss_values = optimisation_fn(X, Y, network, batch_size, learning_rate, max_epochs, loss_fn)\n",
        "\n",
        "  plt.plot(loss_values)\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "trainy_onehot = []\n",
        "for y in trainy:\n",
        "  curr_y = [0]*N_CLASSES\n",
        "  curr_y[y] = 1\n",
        "  trainy_onehot.append(curr_y)\n",
        "\n",
        "trainy_onehot = np.array(trainy_onehot)\n",
        "\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "train_NN(trainX_reshaped.T, trainy_onehot, momentum_gradient_descent, 256, 1e-3, 200, 3, 32, random_initialisation, sigmoid)\n",
        "run.finish()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1tso3aho) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 720<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7998333b253a4d8aaf8a3129f5e69f09",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210307_085341-1tso3aho/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210307_085341-1tso3aho/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>7252</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>loss</td><td>2.3043</td></tr><tr><td>_runtime</td><td>838</td></tr><tr><td>_timestamp</td><td>1615108062</td></tr><tr><td>_step</td><td>7251</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>loss</td><td>█▃▆▃█▁▄▅▄▂▄▇▆▄▄▄▅▄▃▃▃▄▆▇▄▅▆▄▅▅▃▃▃▄▄▃▃▅▄▃</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">daily-planet-53</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1tso3aho\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1tso3aho</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1tso3aho). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">soft-wildflower-54</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/21q97sd6\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/21q97sd6</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210307_090806-21q97sd6</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/200 [00:26<1:26:22, 26.04s/it]\u001b[A\n",
            "  1%|          | 2/200 [00:51<1:25:48, 26.00s/it]\u001b[A\n",
            "  2%|▏         | 3/200 [01:17<1:24:55, 25.87s/it]\u001b[A\n",
            "  2%|▏         | 4/200 [01:42<1:24:02, 25.72s/it]\u001b[A\n",
            "  2%|▎         | 5/200 [02:08<1:23:11, 25.60s/it]\u001b[A\n",
            "  3%|▎         | 6/200 [02:33<1:22:17, 25.45s/it]\u001b[A\n",
            "  4%|▎         | 7/200 [02:58<1:21:56, 25.47s/it]\u001b[A\n",
            "  4%|▍         | 8/200 [03:24<1:21:26, 25.45s/it]\u001b[A\n",
            "  4%|▍         | 9/200 [03:49<1:20:46, 25.38s/it]\u001b[A\n",
            "  5%|▌         | 10/200 [04:14<1:19:50, 25.21s/it]\u001b[A\n",
            "  6%|▌         | 11/200 [04:39<1:19:10, 25.13s/it]\u001b[A\n",
            "  6%|▌         | 12/200 [05:04<1:18:30, 25.06s/it]\u001b[A\n",
            "  6%|▋         | 13/200 [05:29<1:18:09, 25.08s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-14ac634ae2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"assignment1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abisheks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_gradient_descent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_initialisation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-14ac634ae2b3>\u001b[0m in \u001b[0;36mtrain_NN\u001b[0;34m(X, Y, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn, activation_fn, pre_activation_fn, output_fn, grad_act_fn, grad_output_fn, loss_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_act_fns_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_fns_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_act_fns_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_initialisation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimisation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f2262154e0cb>\u001b[0m in \u001b[0;36mmomentum_gradient_descent\u001b[0;34m(X, Y, network, batch_size, eta, max_epochs, loss_fn, beta)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mcurr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mgrad_w_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-de8ce0656f09>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(network, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mH_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedforward_neuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'biases'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activation_fn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_activation_fn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cache'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcache_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c5289a7048f5>\u001b[0m in \u001b[0;36mfeedforward_neuron\u001b[0;34m(H_prev, W, b, activation_fn, pre_activation_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mH_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-c5289a7048f5>\u001b[0m in \u001b[0;36mpre_activation\u001b[0;34m(H_prev, W, b, pre_activation_fn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpre_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# Calculates the pre-activation output and caches the required values. Returns the output and cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_activation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2323ab00cac6>\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(W, x, b)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Calculates the linear function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}