{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc60056564ae4274aba6fa753e4ed551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dad8d3a61f834f9f897f3ec1e2ea59af",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f93b9b31a7574b90af3b9887c65e63ef",
              "IPY_MODEL_430ec3c5826440038eef7ed09016b3da"
            ]
          }
        },
        "dad8d3a61f834f9f897f3ec1e2ea59af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f93b9b31a7574b90af3b9887c65e63ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_8e0120f102494136a2046e97e22900a8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6389511423de435f9cb1f2da6de05dc8"
          }
        },
        "430ec3c5826440038eef7ed09016b3da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_86b239f91aa44220bf2cefd303126a09",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0faa65f1548c44029724c982ef58cd93"
          }
        },
        "8e0120f102494136a2046e97e22900a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6389511423de435f9cb1f2da6de05dc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86b239f91aa44220bf2cefd303126a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0faa65f1548c44029724c982ef58cd93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad180aa-5992-4a74-f9c7-f76267c7dd72"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.7/dist-packages (0.10.21)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d6fc4f-e3a4-4bce-91bb-c4ceaf810fac"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "# getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)\n",
        "# run.finish()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def relu(x):\n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def grad_relu(x):\n",
        "  return x>0\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def softmax(x):\n",
        "  # Calculates the softmax function\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def linear(W, x, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ x + b\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def grad_tanh(x):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(y_pred, y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(y - y_pred)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising randomly structure with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "trainX_reshaped = trainX.reshape(trainX.shape[0], (trainX.shape[1]*trainX.shape[2])) / 255  # Input Training data with ith column being ith training example's data\n",
        "testX_reshaped = testX.reshape(testX.shape[0], (testX.shape[1]*testX.shape[2])) / 255       # Input Training data with ith column being ith training example's data\n",
        "\n",
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "# wandb.config.update({\"n_hidden_layers\": 3, \"size_hidden_layer\":32})    # Setting the hyperparameters in the wandb\n",
        "# L = wandb.config['n_hidden_layers']+1                                 # Number of hidden layerws + Output layer in the neural network\n",
        "# n_L = [wandb.config['size_hidden_layer']] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "# n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "# n_L[L] = N_CLASSES\n",
        "\n",
        "# pre_act_fns_L = [linear] * L                  # List of Pre-activation functions of the hidden layers and output layer\n",
        "# act_fns_L = [sigmoid] * (L-1) + [softmax]     # List of Activation functions of the hidden layers and output layer\n",
        "# grad_act_fns_L = [grad_sigmoid] * (L-1)       # List of Gradients of the Activation functions, of the hidden layers\n",
        "# grad_output_fn = Softmax_CrossEntropy_grad\n",
        "\n",
        "# network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, random_initialisation)\n",
        "# print(network)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = A   # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  H_prev = H_prev.reshape((H_prev.shape[0], 1))\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  H = H.reshape((H.shape[0],1))\n",
        "  cache = (pre_activation_cache, H_prev)   # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, x):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      x -- Input data from the training set\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    H = x                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "    cache_prev = (x,x) \n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        #network[l]['cache'] = cache_prev\n",
        "        network[l]['cache'] = cache\n",
        "        cache_prev = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "            \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, y, y_pred):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      y -- True output corresponding to the training data input\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](y_pred, y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = grad_a_k_L @ network[k]['cache'][1].T\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = grad_a_k_L\n",
        "\n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k-1]['cache'][0])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(y_pred, y_true):\n",
        "  return -(y_true * np.log(y_pred)).sum()\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "  return ((y_true - y_pred) ** 2).sum()\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def vanilla_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db = [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          network[k]['weights'] -= eta * dw[k]\n",
        "          network[k]['biases'] -= eta * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "  \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "  \n",
        "  return loss_values\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "  \n",
        "  return loss_values\n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  lookahead_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred_org = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred_org, y)\n",
        "      y_pred = forward_propagation(lookahead_network, x)\n",
        "      grad_w_L, grad_b_L = back_propagation(lookahead_network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k] \n",
        "          lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "          lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k]\n",
        "        lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "        lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "\n",
        "  return loss_values\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def RMSProp_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "  \n",
        "  return loss_values"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 50, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  return loss_values\n",
        "  "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lk1FkKdZj1"
      },
      "source": [
        "# Nadam optimisation followed from this paper : https://arxiv.org/pdf/1609.04747.pdf\n",
        "def nadam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nadam Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  return loss_values\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc60056564ae4274aba6fa753e4ed551",
            "dad8d3a61f834f9f897f3ec1e2ea59af",
            "f93b9b31a7574b90af3b9887c65e63ef",
            "430ec3c5826440038eef7ed09016b3da",
            "8e0120f102494136a2046e97e22900a8",
            "6389511423de435f9cb1f2da6de05dc8",
            "86b239f91aa44220bf2cefd303126a09",
            "0faa65f1548c44029724c982ef58cd93"
          ]
        },
        "outputId": "9ae2da93-a661-4266-a824-68f47865b2cc"
      },
      "source": [
        "def print_accuracy(network):\n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "\n",
        "def train_NN(X, Y, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn, activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss):\n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"max_epochs\": max_epochs\n",
        "                      })\n",
        "    \n",
        "  L = wandb.config['no_hidden_layers']+1                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [wandb.config['size_hidden_layer']] * (L+1)     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = X.shape[1]\n",
        "  n_L[L] = Y.shape[1]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  loss_values = optimisation_fn(X, Y, network, batch_size, learning_rate, max_epochs, loss_fn)\n",
        "\n",
        "  plt.plot(loss_values)\n",
        "  plt.show()  \n",
        "\n",
        "  return network\n",
        "\n",
        "trainy_onehot = []\n",
        "for y in trainy:\n",
        "  curr_y = [0]*N_CLASSES\n",
        "  curr_y[y] = 1\n",
        "  trainy_onehot.append(curr_y)\n",
        "\n",
        "trainy_onehot = np.array(trainy_onehot)\n",
        "\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "network = train_NN(trainX_reshaped, trainy_onehot, nesterov_gradient_descent, 128, 1e-1, 30, 2, 10, random_initialisation, sigmoid,  linear, softmax, grad_sigmoid)\n",
        "print_accuracy(network)\n",
        "run.finish()\n",
        "\n",
        "print(f'Training data original output : {trainy_onehot[0]}')\n",
        "print(f'Training data output from NN : {forward_propagation(network, trainX_reshaped[0].reshape((trainX_reshaped.shape[1],1))).T}')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">breezy-meadow-135</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1h9j2ftx\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1h9j2ftx</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210308_053335-1h9j2ftx</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [04:50<00:00,  9.67s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.8369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.8369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 292<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc60056564ae4274aba6fa753e4ed551",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210308_053335-1h9j2ftx/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210308_053335-1h9j2ftx/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>14070</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.27916</td></tr><tr><td>_runtime</td><td>292</td></tr><tr><td>_timestamp</td><td>1615181907</td></tr><tr><td>_step</td><td>14069</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▁▂▃▂▂▂▂▂▂▂▃▂▂▁▁▂▂▂▂▁▂▂▂▁▂▂▃▂▂▂▃▂▂▃</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">breezy-meadow-135</strong>: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1h9j2ftx\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1h9j2ftx</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training data original output : [0 0 0 0 0 0 0 0 0 1]\n",
            "Training data output from NN : [[9.01988603e-06 1.72196034e-11 1.45653781e-07 1.86219079e-08\n",
            "  5.96563658e-08 1.97129669e-03 4.34257175e-07 1.26732659e-02\n",
            "  6.98472888e-05 9.85275912e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8178a8-bd3c-4e70-b688-d8d9035fb5e3"
      },
      "source": [
        "print(\"Accuracy = %0.4f\"%5.0)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 5.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYFa9Jh7rU0"
      },
      "source": [
        ""
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}