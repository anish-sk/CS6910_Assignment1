{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish-sk/CS6910_Assignment1/blob/master/src/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DMWBsOgIyM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4ba2ad-fd13-4a94-cb61-1ccd0e7c44ca"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 6746f968d95eb71e281d6c7772a0469574430408"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/17/b1e27f77c3d47f6915a774ecf632e3f5a7d49d9fa3991547729e7f19bedd/wandb-0.10.21-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 8.6MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=58b132aa8f22867740ac07c333ac6978fd194177ff3f8e4e999a89e81dea5376\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=6c02e8bf2ca04589468851c1af30ecacd114b876f6e541e26dd95fe33612dccf\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, shortuuid, docker-pycreds, configparser, pathtools, smmap, gitdb, GitPython, sentry-sdk, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2ZVTXmJfT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1cd088-a61c-414b-ecc7-59a4a8e35a5d"
      },
      "source": [
        "# Init wandb\n",
        "import wandb\n",
        "\n",
        "# run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "# Loading the fashion mnist dataset\n",
        "from keras.datasets import fashion_mnist\n",
        "# Setting seed value\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load dataset (train data and test data)\n",
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtNK58VvJrZO"
      },
      "source": [
        "# Number of classes in the Fashion-MNIST dataset\n",
        "N_CLASSES = np.unique(trainy).shape[0]    # 10 as known from the keras documentation\n",
        "\n",
        "# Captions/Labels for the output classes present in Fashion-MNIST dataset\n",
        "IMG_LABELS = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def getSampleImages(nClass, imgLabels, X, y, nSamples):\n",
        "  '''\n",
        "  The function takes few samples of each class from the dataset provided and passes it to the WANDB for it log the images\n",
        "\n",
        "  Arguments :\n",
        "    nClass -- Number of output classes in the dataset\n",
        "    imgLabels -- List of labels for the output classes (numbered from 0 to nClass - 1)\n",
        "    X -- The input data containing images in the form of matrices\n",
        "    y -- The output data containing the class to which an input belongs\n",
        "    nSamples -- Number of samples of each class to be taken. If that many samples not present in dataset, maximum number of samples present (from that class) will be taken\n",
        "\n",
        "  Returns :\n",
        "    -- None --\n",
        "  '''\n",
        "\n",
        "  # Initialise empty list to store the input data sampled from each class\n",
        "  sampleImgsX = [[] for _ in range(nClass)]\n",
        "\n",
        "  # Take 3 sample images from each class\n",
        "  for i in range(y.shape[0]):\n",
        "    if len(sampleImgsX[y[i]]) < nSamples :\n",
        "      sampleImgsX[y[i]].append(X[i])\n",
        "\n",
        "\n",
        "  # Getting a list of sample images of each class to be saved to wandb\n",
        "  sampleImgsList = []\n",
        "  for i in range(nClass):\n",
        "    for j in range(3):\n",
        "      sampleImgsList.append(wandb.Image(sampleImgsX[i][j], caption = imgLabels[i]))\n",
        "\n",
        "  np.random.shuffle(sampleImgsList)\n",
        "  wandb.log({\"example\" : sampleImgsList})\n",
        "\n",
        "\n",
        "# Question 1 : Show 3 sample images from training set of downloaded Fashion-MNIST dataset in WANDB\n",
        "# getSampleImages(N_CLASSES, IMG_LABELS, trainX, trainy, 3)\n",
        "# run.finish()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbufY1DHNQdm"
      },
      "source": [
        "def relu(x):\n",
        "  return np.maximum(x,0)\n",
        "\n",
        "def grad_relu(x):\n",
        "  return x>0\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Calculates the sigmoid function\n",
        "  return np.exp(-np.logaddexp(0, -x))\n",
        "\n",
        "def softmax(x):\n",
        "  # Calculates the softmax function\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "\n",
        "def linear(W, x, b):\n",
        "  # Calculates the linear function\n",
        "  return W @ x + b\n",
        "\n",
        "def grad_sigmoid(x):\n",
        "  # Calculates the gradient of sigmoid function\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def grad_tanh(x):\n",
        "  # Calculates the gradient of tanh function\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "def Softmax_CrossEntropy_grad(y_pred, y):\n",
        "  # Calculates the gradient of the output layer with softmax activation and cross entropy loss\n",
        "  # layer -- The dictionary for the output layer contianing info about it\n",
        "  # y -- True output\n",
        "  return -(y - y_pred)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX_08I3AJaiF"
      },
      "source": [
        "def random_initialisation(shape):\n",
        "  # Initialising randomly structure with given dimensions (shape) as tuple\n",
        "  return np.random.randn(*shape)*0.1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3rWXP7YMDOS"
      },
      "source": [
        "trainX_reshaped = trainX.reshape(trainX.shape[0], (trainX.shape[1]*trainX.shape[2])) / 255  # Input Training data with ith column being ith training example's data\n",
        "testX_reshaped = testX.reshape(testX.shape[0], (testX.shape[1]*testX.shape[2])) / 255       # Input Training data with ith column being ith training example's data\n",
        "\n",
        "def initialize_network(n_L, preActFns_L, actFns_L, gradActFns_L, gradOutputFn, weight_initialisation):\n",
        "  '''\n",
        "  The function initializes the neural network and the appropriate parameters\n",
        "  \n",
        "  Arguments :\n",
        "    n_L -- an array whose ith element represents the number of neurons in the ith layer (0 - Input Layer, last element - Output Layer)\n",
        "    preActFns_L -- an array who ith element is the Pre Activation function of the (i+1)th layer of the neural network\n",
        "    actFns_L -- an array who ith element is the Activation function of the (i+1)th layer of the neural network\n",
        "    gradActFns_L -- an array who ith element is the gradient of the Activation function of the (i+1)th layer of the neural network\n",
        "    gradOutputFn -- Function to calculate gradients wrt a_L (output layer) in back-propagation\n",
        "    weight_initialisation -- Function to initialise weights of the layers\n",
        "  \n",
        "  Returns :\n",
        "    network -- the initialized network as an array of dictionaries for the hidden and output layers of the neural network\n",
        "  '''\n",
        "\n",
        "  L = len(n_L)-1\n",
        "\n",
        "  assert(L >= 1)\n",
        "  assert(len(preActFns_L) == L)\n",
        "  assert(len(actFns_L) == L)\n",
        "\n",
        "  network = list()\n",
        "  for i in range(1,L+1):\n",
        "    # Dictionary for each layer representing it's constituents\n",
        "    layer = {'weights':weight_initialisation((n_L[i],n_L[i-1])),  # Weight matrix for (i-1)th to ith layer transition\n",
        "             'biases':np.zeros((n_L[i],1)),                       # Bias vector for (i-1)th to ith layer transition\n",
        "             'pre_activation_fn':preActFns_L[i-1],                # Pre-activation function for neurons of the ith layer\n",
        "             'activation_fn':actFns_L[i-1],                       # Activation function for neurons of the ith layer             \n",
        "             'no_neurons':n_L[i],                                 # Number of neurons in ith layer\n",
        "             'cache': []                                          # Array of cached pre-activation and activation output for each layer to be used in back-propagation (will be filled in forward-propagation)\n",
        "            }\n",
        "    network.append(layer)\t\n",
        "    if i < L:\n",
        "      network[-1]['grad_activation_fn'] = gradActFns_L[i-1]       # Function calculating Gradient of the Activation function for the ith (hidden) layer\n",
        "  \n",
        "  network[-1]['grad_output_fn'] = gradOutputFn                    # Function calculating Gradient of the Output layer (Gradient of Loss function wrt a_L)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "# wandb.config.update({\"n_hidden_layers\": 3, \"size_hidden_layer\":32})    # Setting the hyperparameters in the wandb\n",
        "# L = wandb.config['n_hidden_layers']+1                                 # Number of hidden layerws + Output layer in the neural network\n",
        "# n_L = [wandb.config['size_hidden_layer']] * (L+1)                     # List of number of neurons in the neural network\n",
        "\n",
        "# n_L[0] = trainX.shape[1] * trainX.shape[2]\n",
        "# n_L[L] = N_CLASSES\n",
        "\n",
        "# pre_act_fns_L = [linear] * L                  # List of Pre-activation functions of the hidden layers and output layer\n",
        "# act_fns_L = [sigmoid] * (L-1) + [softmax]     # List of Activation functions of the hidden layers and output layer\n",
        "# grad_act_fns_L = [grad_sigmoid] * (L-1)       # List of Gradients of the Activation functions, of the hidden layers\n",
        "# grad_output_fn = Softmax_CrossEntropy_grad\n",
        "\n",
        "# network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, random_initialisation)\n",
        "# print(network)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDSj4aTPeTw"
      },
      "source": [
        "def pre_activation(H_prev, W, b, pre_activation_fn):\n",
        "  # Calculates the pre-activation output and caches the required values. Returns the output and cache.\n",
        "  A = pre_activation_fn(W, H_prev, b)\n",
        "  \n",
        "  assert(A.shape[0] == W.shape[0])\n",
        "  pre_act_cache = A   # Caching the pre-activation ouptut to be used in backpropagation\n",
        "\n",
        "  return A, pre_act_cache\n",
        "\n",
        "def feedforward_neuron(H_prev, W, b, activation_fn, pre_activation_fn):\n",
        "  # Calculates the activation output (using the pre-activation function above) and caches the required values. Returns the output and cache.\n",
        "\n",
        "  H_prev = H_prev.reshape((H_prev.shape[0], 1))\n",
        "  A, pre_activation_cache = pre_activation(H_prev, W, b, pre_activation_fn)\n",
        "  H = activation_fn(A)\n",
        "  \n",
        "  assert (H.shape[0] == W.shape[0])\n",
        "  H = H.reshape((H.shape[0],1))\n",
        "  cache = (pre_activation_cache, H)   # Caching the pre-activation and activation output to use it in back-propagation\n",
        "\n",
        "  return H, cache"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVFXPTolPf5v"
      },
      "source": [
        "def forward_propagation(network, x):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      x -- Input data from the training set\n",
        "    \n",
        "    Returns :\n",
        "      Output from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    H = x                         # Initialising H to input\n",
        "    L = len(network)              # Number of (hidden + output) layers in the neural network\n",
        "    cache_prev = (x,x) \n",
        "    for l in range(0, L):\n",
        "        H_prev = H \n",
        "        H, cache = feedforward_neuron(H_prev, network[l]['weights'], network[l]['biases'], network[l]['activation_fn'], network[l]['pre_activation_fn'])\n",
        "        network[l]['cache'] = cache_prev\n",
        "        cache_prev = cache\n",
        "    \n",
        "    assert(H.shape[0] == (network[L-1]['no_neurons']))\n",
        "            \n",
        "    return H\n",
        "\n",
        "# HL = forward_propagation(trainX_reshaped, network)          # HL -- output from the neural network\n",
        "# print(HL)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQvQGxcaCTC"
      },
      "source": [
        " def back_propagation(network, y, y_pred):\n",
        "  \"\"\"\n",
        "    Implement backward propagation for the given neural network\n",
        "    \n",
        "    Arguments :\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      y -- True output corresponding to the training data input\n",
        "    \n",
        "    Returns :\n",
        "      H -- Output from the neural network\n",
        "  \"\"\"\n",
        "\n",
        "  L = len(network)\n",
        "\n",
        "  # Gradients wrt output layer (a_L)\n",
        "  grad_a_k_L = network[L-1]['grad_output_fn'](y_pred, y)\n",
        "\n",
        "  # Initialising gradients to be calculated in the loop below\n",
        "  grad_w_L = [np.zeros(2)] * L\n",
        "  grad_b_L = [np.zeros(2)] * L\n",
        "  grad_h_prev_L, grad_a_prev_L = 0, 0\n",
        "\n",
        "\n",
        "  for k in range(L-1,-1,-1):\n",
        "    # Gradients wrt Weights (W_k)\n",
        "    grad_w_L[k] = grad_a_k_L @ network[k]['cache'][1].T\n",
        "\n",
        "    # Gradients wrt Biases (b_k)\n",
        "    grad_b_L[k] = grad_a_k_L\n",
        "\n",
        "    # Gradients wrt hidden layer\n",
        "    # Gradients wrt h_(k-1)\n",
        "    grad_h_prev_L = network[k]['weights'].T @ grad_a_k_L\n",
        "\n",
        "    # Gradients wrt a_(k-1)\n",
        "    if(k > 0):\n",
        "      grad_act_fn_prev = network[k-1]['grad_activation_fn'](network[k]['cache'][0])\n",
        "      grad_a_prev_L = grad_h_prev_L * grad_act_fn_prev\n",
        "\n",
        "    grad_a_k_L = grad_a_prev_L\n",
        "\n",
        "  return grad_w_L, grad_b_L"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyD4oPSIL19s"
      },
      "source": [
        "def CrossEntropy_loss(y_pred, y_true):\n",
        "  return -(y_true * np.log(y_pred)).sum()\n",
        "\n",
        "def SquaredError(y_pred, y_true):\n",
        "  return ((y_true - y_pred) ** 2).sum()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZpq0rM0Vvf"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def vanilla_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Vanilla/Batch Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db = [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          network[k]['weights'] -= eta * dw[k]\n",
        "          network[k]['biases'] -= eta * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "  \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "  \n",
        "  return loss_values\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQkfH0-oz4A"
      },
      "source": [
        "def momentum_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Momentum Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= eta * dw[k]\n",
        "        network[k]['biases'] -= eta * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "  \n",
        "  return loss_values\n",
        "\n",
        "def nesterov_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nesterov Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      beta -- Hyperparameter to tune dependency of gradient on history\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  lookahead_network = network[:]\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):\n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred_org = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred_org, y)\n",
        "      y_pred = forward_propagation(lookahead_network, x)\n",
        "      grad_w_L, grad_b_L = back_propagation(lookahead_network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "          m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "          network[k]['weights'] -= m_w[k]\n",
        "          network[k]['biases'] -= m_b[k] \n",
        "          lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "          lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta + dw[k] * eta\n",
        "        m_b[k] = m_b[k] * beta + db[k] * eta\n",
        "        network[k]['weights'] -= m_w[k]\n",
        "        network[k]['biases'] -= m_b[k]\n",
        "        lookahead_network[k]['weights'] -= (eta * dw[k] + beta * m_w[k])\n",
        "        lookahead_network[k]['biases'] -= (eta * db[k] + beta * m_b[k])\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "\n",
        "  return loss_values\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2eWRLqPUiw"
      },
      "source": [
        "def RMSProp_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta = 0.9):\n",
        "  \"\"\"\n",
        "    Trains the neural network using RMSProp Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        v_w[k] = v_w[k] * beta + (1-beta) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta + (1-beta) * db[k]**2\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w[k] + eps)) * dw[k]\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b[k] + eps)) * db[k]\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "  \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "  \n",
        "  return loss_values"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OhU1Zf9owIh"
      },
      "source": [
        "def adam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 50, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Adam Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "\n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "\n",
        "  return loss_values\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9lk1FkKdZj1"
      },
      "source": [
        "# Nadam optimisation followed from this paper : https://arxiv.org/pdf/1609.04747.pdf\n",
        "def nadam_gradient_descent(X, Y, network, batch_size = 1, eta = 5e-2, max_epochs = 100, loss_fn = CrossEntropy_loss, eps = 1e-8, beta1 = 0.9, beta2 = 0.999):\n",
        "  \"\"\"\n",
        "    Trains the neural network using Nadam Accelerated Gradient Descent\n",
        "    \n",
        "    Arguments :\n",
        "      X -- Input data matrix where ith row is the input data corresponding to ith training example\n",
        "      Y -- True output matrix where ith row is the output data corresponding to ith training example\n",
        "      network -- Given neural network (as an array of dictionaries)\n",
        "      eta -- Learning Rate\n",
        "      max_epochs -- Number of iterations for convergence\n",
        "      loss_fn -- Function used to calculate the loss in the neural network\n",
        "      eps -- Epsilon hyperparameter\n",
        "      beta1 -- Hyperparameter acting as decaying weight for momentum update\n",
        "      beta2 -- Hyperparameter acting as decaying weight for learning rate update\n",
        "      \n",
        "    Returns :\n",
        "      -- None --\n",
        "  \"\"\"\n",
        "  loss_values = []\n",
        "  L = len(network)\n",
        "  M = X.shape[0]\n",
        "  curr_loss, batch = 0, 0\n",
        "\n",
        "  dw, db, m_w, m_b, v_w, v_b = [0] * L, [0] * L, [0] * L, [0] * L, [0] * L, [0] * L\n",
        "  for k in range(L):\n",
        "    dw[k] = np.zeros_like(network[k]['weights'])\n",
        "    db[k] = np.zeros_like(network[k]['biases'])\n",
        "    m_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    m_b[k] = np.zeros_like(network[k]['biases'])\n",
        "    v_w[k] = np.zeros_like(network[k]['weights'])\n",
        "    v_b[k] = np.zeros_like(network[k]['biases'])\n",
        "\n",
        "\n",
        "  for epoch in tqdm(range(max_epochs)):  \n",
        "\n",
        "    counter = 0\n",
        "    for x,y in zip(X, Y):\n",
        "      counter += 1\n",
        "      x = x.reshape((x.shape[0], 1))\n",
        "      y = y.reshape((y.shape[0], 1))\n",
        "      y_pred = forward_propagation(network, x)\n",
        "      curr_loss += loss_fn(y_pred, y)\n",
        "      grad_w_L, grad_b_L = back_propagation(network, y, y_pred)\n",
        "      for k in range(L):\n",
        "        dw[k] += grad_w_L[k]\n",
        "        db[k] += grad_b_L[k]\n",
        "      \n",
        "      if counter % batch_size == 0:\n",
        "        batch += 1\n",
        "        curr_loss /= batch_size\n",
        "        loss_values.append(curr_loss)\n",
        "        wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "        for k in range(L):\n",
        "          dw[k] /= batch_size\n",
        "          db[k] /= batch_size\n",
        "          m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "          m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "          v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "          v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "          m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "          m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "          v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "          v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "          network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "          network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "          dw[k] = np.zeros_like(network[k]['weights'])\n",
        "          db[k] = np.zeros_like(network[k]['biases'])\n",
        "          curr_loss = 0\n",
        "\n",
        "    if counter % batch_size != 0:\n",
        "      batch += 1\n",
        "      size = counter % batch_size\n",
        "      curr_loss /= size\n",
        "      loss_values.append(curr_loss)\n",
        "      wandb.log({'batch': batch, 'epoch': epoch, 'loss': curr_loss})\n",
        "      for k in range(L):\n",
        "        dw[k] /= size\n",
        "        db[k] /= size\n",
        "        m_w[k] = m_w[k] * beta1 + (1-beta1) * dw[k]\n",
        "        m_b[k] = m_b[k] * beta1 + (1-beta1) * db[k]\n",
        "        v_w[k] = v_w[k] * beta2 + (1-beta2) * dw[k]**2\n",
        "        v_b[k] = v_b[k] * beta2 + (1-beta2) * db[k]**2\n",
        "        m_w_hat = m_w[k] / (1 - math.pow(beta1, batch))\n",
        "        m_b_hat = m_b[k] / (1 - math.pow(beta1, batch))\n",
        "        v_w_hat = v_w[k] / (1 - math.pow(beta2, batch))\n",
        "        v_b_hat = v_b[k] / (1 - math.pow(beta2, batch))\n",
        "        network[k]['weights'] -= (eta / np.sqrt(v_w_hat + eps)) * (m_w_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * dw[k])\n",
        "        network[k]['biases'] -= (eta / np.sqrt(v_b_hat + eps)) * (m_b_hat + (1-beta1) / (1 - math.pow(beta1, batch)) * db[k])\n",
        "        dw[k] = np.zeros_like(network[k]['weights'])\n",
        "        db[k] = np.zeros_like(network[k]['biases'])\n",
        "        curr_loss = 0\n",
        "        \n",
        "  n_correct = 0\n",
        "  for tx, ty in zip(testX_reshaped, testy):\n",
        "    tx = tx.reshape((tx.shape[0], 1))\n",
        "    testy_pred = np.argmax(forward_propagation(network, tx))\n",
        "    n_correct += (testy_pred == ty)\n",
        "\n",
        "  print(\"Accuracy = %0.4f\"%(n_correct/testX.shape[0]))\n",
        "\n",
        "  return loss_values\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy7GHtEA3Kwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f2701770-56ea-41c6-dda2-03b91b7ff125"
      },
      "source": [
        "def train_NN(X, Y, optimisation_fn, batch_size, learning_rate, max_epochs, no_hidden_layers, size_hidden_layer, weight_initialisation_fn, activation_fn, pre_activation_fn = linear, output_fn = softmax, grad_act_fn = grad_sigmoid, grad_output_fn = Softmax_CrossEntropy_grad, loss_fn = CrossEntropy_loss):\n",
        "  # Setting the hyperparameters in the wandb\n",
        "  wandb.config.update({\"no_hidden_layers\": no_hidden_layers, \n",
        "                       \"size_hidden_layer\": size_hidden_layer,\n",
        "                       \"batch_size\": batch_size,\n",
        "                       \"learning_rate\": learning_rate,\n",
        "                       \"max_epochs\": max_epochs\n",
        "                      })\n",
        "    \n",
        "  L = wandb.config['no_hidden_layers']+1                # Number of hidden layerws + Output layer in the neural network\n",
        "  n_L = [wandb.config['size_hidden_layer']] * (L+1)     # List of number of neurons in the neural network\n",
        "\n",
        "  n_L[0] = X.shape[1]\n",
        "  n_L[L] = Y.shape[1]\n",
        "\n",
        "  pre_act_fns_L = [pre_activation_fn] * L               # List of Pre-activation functions of the hidden layers and output layer\n",
        "  act_fns_L = [activation_fn] * (L-1) + [output_fn]     # List of Activation functions of the hidden layers and output layer\n",
        "  grad_act_fns_L = [grad_act_fn] * (L-1)                # List of Gradients of the Activation functions, of the hidden layers\n",
        "\n",
        "  network = initialize_network(n_L, pre_act_fns_L, act_fns_L, grad_act_fns_L, grad_output_fn, weight_initialisation_fn)\n",
        "  loss_values = optimisation_fn(X, Y, network, batch_size, learning_rate, max_epochs, loss_fn)\n",
        "\n",
        "  plt.plot(loss_values)\n",
        "  plt.show()  \n",
        "\n",
        "\n",
        "trainy_onehot = []\n",
        "for y in trainy:\n",
        "  curr_y = [0]*N_CLASSES\n",
        "  curr_y[y] = 1\n",
        "  trainy_onehot.append(curr_y)\n",
        "\n",
        "trainy_onehot = np.array(trainy_onehot)\n",
        "\n",
        "run = wandb.init(project=\"assignment1\", entity=\"abisheks\", reinit=True)\n",
        "train_NN(trainX_reshaped, trainy_onehot, RMSProp_gradient_descent, 128, 1e-3, 30, 2, 10, random_initialisation, sigmoid,  linear, softmax, grad_sigmoid)\n",
        "run.finish()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">noble-microwave-126</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/abisheks/assignment1\" target=\"_blank\">https://wandb.ai/abisheks/assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/abisheks/assignment1/runs/1we0fu14\" target=\"_blank\">https://wandb.ai/abisheks/assignment1/runs/1we0fu14</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210307_210814-1we0fu14</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 3/30 [00:24<03:40,  8.16s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5lxNGplX5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a953af26-0ca2-4898-8bd0-4c66c08fce0f"
      },
      "source": [
        "print(\"Accuracy = %0.4f\"%5.0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 5.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYFa9Jh7rU0"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}